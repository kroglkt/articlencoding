{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for extracting features for authorship attribution.\n",
    "In this notebook all authorship attribution features will be collected. They will be saved after extraction. Make sure, you define each feature extraction in a function, so it easily can be repurposed.\n",
    "\n",
    "Author: lkt259@alumni.ku.dk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence length\n",
    "Nice feature, very complex!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"This list has overlapping features with content features. For example, word n-grams will capture the content of the text along with stylometric tendencies. Content features consist of word frequencies, word and character n-grams, hapax legomena etc. This overlap is not of concern, however, as Sari et al. \\cite{Sari2018} show, using content features is beneficial when performing authorship attribution of news articles because journalists often have certain topics they prefer writing about. They argue that using only stylometric features is beneficial when attributing authors to texts of the same topic or genre, e.g. law text or movie reviews.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats for our test document\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'number_of_sentences': 6,\n",
       " 'avg_sent_len_chars': 107.33333333333333,\n",
       " 'std_sent_len_chars': 48.65410796862093,\n",
       " 'med_sent_len_chars': 95.0,\n",
       " 'avg_sent_len_words': 16.166666666666668,\n",
       " 'std_sent_len_words': 6.618576550554926,\n",
       " 'med_sent_len_words': 14.0}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_sentences(text):\n",
    "    '''Returns an array with text split into sentences'''\n",
    "    return np.array(re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', text), dtype=str)\n",
    "\n",
    "def remove_dots(word):\n",
    "    return re.sub(r',|\\.|:|!|\\?|;', '', word)\n",
    "\n",
    "def split_words(text):\n",
    "    '''Returns an array with text split into words'''\n",
    "    string = np.array(text.split(), dtype=str)\n",
    "    no_dot = np.array([remove_dots(x) for x in string])\n",
    "    return no_dot\n",
    "\n",
    "def get_sentence_lengths(text):\n",
    "    '''Returns dictionary with sentence lengh in chars and words'''\n",
    "    split_text = split_sentences(text)\n",
    "    num_sentences = len(split_text)\n",
    "    num_chars = np.array([len(x) for x in split_text], dtype=int)\n",
    "    num_words = [split_words(x).size for x in split_sentences(string)]\n",
    "    return {'chars' : num_chars, 'words' : num_words, 'num_sents' : num_sentences}\n",
    "\n",
    "def get_sentence_length_stats(text):\n",
    "    '''Returns dictionary with mean, std and median lengths in both chars and words'''\n",
    "    sentence_lengths = get_sentence_lengths(text)\n",
    "    output = {'number_of_sentences' : sentence_lengths['num_sents'],\n",
    "              'avg_sent_len_chars' : np.mean(sentence_lengths['chars']),\n",
    "              'std_sent_len_chars' : np.std(sentence_lengths['chars']),\n",
    "              'med_sent_len_chars' : np.median(sentence_lengths['chars']),\n",
    "              \n",
    "              'avg_sent_len_words' : np.mean(sentence_lengths['words']),\n",
    "              'std_sent_len_words' : np.std(sentence_lengths['words']),\n",
    "              'med_sent_len_words' : np.median(sentence_lengths['words'])\n",
    "             }\n",
    "    return output\n",
    "\n",
    "print(\"Stats for our test document\")\n",
    "get_sentence_length_stats(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Word length\n",
    "The count of words of the entire text.\n",
    "Also extremely complex feature, cool shit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test word lengths\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'number_of_words': 97,\n",
       " 'avg_word_len_chars': 5.546391752577319,\n",
       " 'std_word_len_chars': 2.853901719013402,\n",
       " 'med_word_len_chars': 5.0}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_word_lengths(split_text):\n",
    "    '''Returns length of words in characters'''\n",
    "    return np.array([len(x) for x in split_text], dtype=int)\n",
    "\n",
    "def get_word_length_stats(text):\n",
    "    '''Returns various stats for words in document'''\n",
    "    #Split text here, to reduce function calls.\n",
    "    split_text = split_words(text)\n",
    "    word_lengths = get_word_lengths(split_text)\n",
    "    output = {\n",
    "        'number_of_words' : len(split_text),\n",
    "        'avg_word_len_chars' : np.mean(word_lengths),\n",
    "        'std_word_len_chars' : np.std(word_lengths),\n",
    "        'med_word_len_chars' : np.median(word_lengths)\n",
    "    }\n",
    "    return output\n",
    "\n",
    "print(\"Test word lengths\")\n",
    "get_word_length_stats(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'.'.isalnum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
