{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for extracting features for authorship attribution.\n",
    "In this notebook all authorship attribution features will be collected. They will be saved after extraction. Make sure, you define each feature extraction in a function, so it easily can be repurposed.\n",
    "\n",
    "Author: lkt259@alumni.ku.dk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence length\n",
    "Nice feature, very complex!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"This list has overlapping features with content features. For example, word n-grams will capture the content of the text along with stylometric tendencies. Content features consist of word frequencies, word and character n-grams, hapax legomena etc. This overlap is not of concern, however, as Sari et al. \\cite{Sari2018} show, using content features is beneficial when performing authorship attribution of news articles because journalists often have certain topics they prefer writing about. They argue that using only stylometric features is beneficial when attributing authors to texts of the same topic or genre, e.g. law text or movie reviews.\"\n",
    "test_corpus = ['This list has overlapping features with content features. For example, word n-grams will capture the content of the text along with stylometric tendencies. Content features consist of word frequencies, word and character n-grams, hapax legomena etc. This overlap is not of concern, however, as Sari et al. \\cite{Sari2018} show, using content features is beneficial when performing authorship attribution of news articles because journalists often have certain topics they prefer writing about. They argue that using only stylometric features is beneficial when attributing authors to texts of the same topic or genre, e.g. law text or movie reviews.',\n",
    "              'Bozkurt et al. \\cite{Bozkurt} performed authorship attribution on Turkish newspaper articles using stylometric features, vocabulary diversity, bag of words and frequency of function words. For stylometric features, they used number of sentences and words in the article, the average number of words in a sentence and the whole article, the vocabulary size, frequencies of symbols used (periods, exclamation marks, etc.) and the number of incomplete sentences. They weighted their features using Term Frequency-Inverse Document Frequency (TF-IDF).',\n",
    "              'TF-IDF is a weighting system often used on words. It consists of two parts: term frequency (TF), which coulds how often a term occurs in a document, and inverse document frequency (IDF) which measures the term importance, as it compares how often the term occurs in a corpus\\cite{TFIDF}. The system is useful for measuring how important a term is for a document. For example, the word \"the\" will often occur in English texts, making it not important even though it occurs frequently in a document. TF-IDF has on many occasions been used in authorship attribution\\cite{Muttenhaler, basile:2019, rahgouy:2019} and can be used on different terms, such as characters, symbols or n-grams.',\n",
    "              'Every year, the research group Webis hosts the PAN shared tasks on digital text forensics and sylometry\\cite{PAN}. Contestants will solve various tasks concerning NLP and on multiple occasions, authorship attribution were part of the tasks. The methodologies used in these tasks are useful resources for feature and model selections.',\n",
    "              'In the overview paper of the authorship attribution task of PAN 2019\\cite{kestemont2019overview}, the 12 best performing models are compiled. All features involve character n-grams and other sorts of n-grams. Other popular choices of n-grams contain words, POS tags, punctuation and distortion. Most of the participants use TF-IDF weighting and SVMs as classifiers. Distortion is a method for masking topical contents of the text before feature extraction, focusing only on stylometric features.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats for our test document\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'number_of_sentences': 6,\n",
       " 'avg_sent_len_chars': 107.33333333333333,\n",
       " 'std_sent_len_chars': 48.65410796862093,\n",
       " 'med_sent_len_chars': 95.0,\n",
       " 'avg_sent_len_words': 16.166666666666668,\n",
       " 'std_sent_len_words': 6.618576550554926,\n",
       " 'med_sent_len_words': 14.0}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_sentences(text):\n",
    "    '''Returns an array with text split into sentences'''\n",
    "    return np.array(re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', text), dtype=str)\n",
    "\n",
    "def remove_dots(word):\n",
    "    return re.sub(r',|\\.|:|!|\\?|;', '', word)\n",
    "\n",
    "def split_words(text):\n",
    "    '''Returns an array with text split into words'''\n",
    "    text = text.lower()\n",
    "    string = np.array(text.split(), dtype=str)\n",
    "    no_dot = np.array([remove_dots(x) for x in string])\n",
    "    return no_dot\n",
    "\n",
    "def get_sentence_lengths(text):\n",
    "    '''Returns dictionary with sentence lengh in chars and words'''\n",
    "    split_text = split_sentences(text)\n",
    "    num_sentences = len(split_text)\n",
    "    num_chars = np.array([len(x) for x in split_text], dtype=int)\n",
    "    num_words = [split_words(x).size for x in split_sentences(string)]\n",
    "    return {'chars' : num_chars, 'words' : num_words, 'num_sents' : num_sentences}\n",
    "\n",
    "def get_sentence_length_stats(text):\n",
    "    '''Returns dictionary with mean, std and median lengths in both chars and words'''\n",
    "    sentence_lengths = get_sentence_lengths(text)\n",
    "    output = {'number_of_sentences' : sentence_lengths['num_sents'],\n",
    "              'avg_sent_len_chars' : np.mean(sentence_lengths['chars']),\n",
    "              'std_sent_len_chars' : np.std(sentence_lengths['chars']),\n",
    "              'med_sent_len_chars' : np.median(sentence_lengths['chars']),\n",
    "              \n",
    "              'avg_sent_len_words' : np.mean(sentence_lengths['words']),\n",
    "              'std_sent_len_words' : np.std(sentence_lengths['words']),\n",
    "              'med_sent_len_words' : np.median(sentence_lengths['words'])\n",
    "             }\n",
    "    return output\n",
    "\n",
    "print(\"Stats for our test document\")\n",
    "get_sentence_length_stats(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Word length\n",
    "The count of words of the entire text.\n",
    "Also extremely complex feature, cool shit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test word lengths\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'number_of_words': 97,\n",
       " 'avg_word_len_chars': 5.546391752577319,\n",
       " 'std_word_len_chars': 2.853901719013402,\n",
       " 'med_word_len_chars': 5.0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_word_lengths(split_text):\n",
    "    '''Returns length of words in characters'''\n",
    "    return np.array([len(x) for x in split_text], dtype=int)\n",
    "\n",
    "def get_word_length_stats(text):\n",
    "    '''Returns various stats for words in document'''\n",
    "    #Split text here, to reduce function calls.\n",
    "    split_text = split_words(text)\n",
    "    word_lengths = get_word_lengths(split_text)\n",
    "    output = {\n",
    "        'number_of_words' : len(split_text),\n",
    "        'avg_word_len_chars' : np.mean(word_lengths),\n",
    "        'std_word_len_chars' : np.std(word_lengths),\n",
    "        'med_word_len_chars' : np.median(word_lengths)\n",
    "    }\n",
    "    return output\n",
    "\n",
    "print(\"Test word lengths\")\n",
    "get_word_length_stats(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word frequency\n",
    "Get word frequencies with TF-IDF weightings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>12</th>\n",
       "      <th>2019</th>\n",
       "      <th>about</th>\n",
       "      <th>al</th>\n",
       "      <th>all</th>\n",
       "      <th>along</th>\n",
       "      <th>and</th>\n",
       "      <th>are</th>\n",
       "      <th>argue</th>\n",
       "      <th>article</th>\n",
       "      <th>...</th>\n",
       "      <th>were</th>\n",
       "      <th>when</th>\n",
       "      <th>which</th>\n",
       "      <th>whole</th>\n",
       "      <th>will</th>\n",
       "      <th>with</th>\n",
       "      <th>word</th>\n",
       "      <th>words</th>\n",
       "      <th>writing</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.091857</td>\n",
       "      <td>0.074110</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.091857</td>\n",
       "      <td>0.04377</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.091857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.183714</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.061518</td>\n",
       "      <td>0.183714</td>\n",
       "      <td>0.222329</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.091857</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083392</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.19701</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.206724</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.103362</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.276891</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 197 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    12  2019     about        al  all     along      and  are     argue  \\\n",
       "0  0.0   0.0  0.091857  0.074110  0.0  0.091857  0.04377  0.0  0.091857   \n",
       "1  0.0   0.0  0.000000  0.083392  0.0  0.000000  0.19701  0.0  0.000000   \n",
       "\n",
       "    article  ...  were      when  which     whole      will      with  \\\n",
       "0  0.000000  ...   0.0  0.183714    0.0  0.000000  0.061518  0.183714   \n",
       "1  0.206724  ...   0.0  0.000000    0.0  0.103362  0.000000  0.000000   \n",
       "\n",
       "       word     words   writing  year  \n",
       "0  0.222329  0.000000  0.091857   0.0  \n",
       "1  0.000000  0.276891  0.000000   0.0  \n",
       "\n",
       "[2 rows x 197 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def train_tfidf_word_freq(corpus):\n",
    "    '''Trains a TF-IDF vectorizer on word frequencies'''\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    \n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    dense = X.todense()\n",
    "    denselist = dense.tolist()\n",
    "    df = pd.DataFrame(denselist, columns=feature_names)\n",
    "    display(df.head(2))\n",
    "    \n",
    "    return X, vectorizer \n",
    "\n",
    "def get_tfidf_word_freq(vectorizer, texts):\n",
    "    '''Returns the TF-IDF weighted word frequencies of test documents'''\n",
    "    #Multiple texts required\n",
    "    return vectorizer.transform(texts)\n",
    "\n",
    "X, vectorizer = train_tfidf_word_freq(test_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hapax legomena\n",
    "Count how many unique words are in a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'features': 5, 'of': 5, 'content': 4, 'word': 3, 'the': 3, 'is': 3, 'this': 2, 'with': 2, 'n-grams': 2, 'text': 2, ...})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.probability.FreqDist(split_words(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
