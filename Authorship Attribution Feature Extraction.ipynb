{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for extracting features for authorship attribution.\n",
    "In this notebook all authorship attribution features will be collected. They will be saved after extraction. Make sure, you define each feature extraction in a function, so it easily can be repurposed.\n",
    "\n",
    "Author: lkt259@alumni.ku.dk & vsl133@alumni.ku.dk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from lexicalrichness import LexicalRichness\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "import spacy\n",
    "import scipy\n",
    "import pickle\n",
    "import os, sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_emot():\n",
    "    e = [\"(o_o)\",\":-)\",\":P\",\":D\",\"x)\",\"ᓚᘏᗢ\",\"╯°□°）╯︵ ┻━┻\",\":)\",\n",
    "         \"*<:-)\",\"^_^\",\"(⌐■_■)\",\"¯\\_(ツ)_/¯\", \"(T_T)\",\":o\",\"OwO\",\n",
    "        \"( ͡❛ ͜ʖ ͡❛)\",\"(̶◉͛‿◉̶)\",\"( ≖.≖)\",\"(ㆆ_ㆆ)\",\"ʕ•́ᴥ•̀ʔっ\",\"( ◡́.◡̀)\",\"(^◡^ )\"]\n",
    "    return random.choice(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence length\n",
    "Nice feature, very complex!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"We can work with single sentences (classifying them with respect to sentiment, topic, authorship, etc), or more than one at a time (checking for similarities, contradiction, question/answer pairs, etc.) Another successful application is to encode one sentence in one language and use a different autoencoder to decode it into another language, e.g. Cho et al. (2014).\"\n",
    "train_corpus = ['This list has overlapping features with content features. For example, word n-grams will capture the content of the text along with stylometric tendencies. Content features consist of word frequencies, word and character n-grams, hapax legomena etc. This overlap is not of concern, however, as Sari et al. \\cite{Sari2018} show, using content features is beneficial when performing authorship attribution of news articles because journalists often have certain topics they prefer writing about. They argue that using only stylometric features is beneficial when attributing authors to texts of the same topic or genre, e.g. law text or movie reviews.',\n",
    "              'Bozkurt et al. \\cite{Bozkurt} performed authorship attribution on Turkish newspaper articles using stylometric features, vocabulary diversity, bag of words and frequency of function words. For stylometric features, they used number of sentences and words in the article, the average number of words in a sentence and the whole article, the vocabulary size, frequencies of symbols used (periods, exclamation marks, etc.) and the number of incomplete sentences. They weighted their features using Term Frequency-Inverse Document Frequency (TF-IDF).',\n",
    "              'TF-IDF is a weighting system often used on words. It consists of two parts: term frequency (TF), which coulds how often a term occurs in a document, and inverse document frequency (IDF) which measures the term importance, as it compares how often the term occurs in a corpus\\cite{TFIDF}. The system is useful for measuring how important a term is for a document. For example, the word \"the\" will often occur in English texts, making it not important even though it occurs frequently in a document. TF-IDF has on many occasions been used in authorship attribution\\cite{Muttenhaler, basile:2019, rahgouy:2019} and can be used on different terms, such as characters, symbols or n-grams.',\n",
    "              'Every year, the research group Webis hosts the PAN shared tasks on digital text forensics and sylometry\\cite{PAN}. Contestants will solve various tasks concerning NLP and on multiple occasions, authorship attribution were part of the tasks. The methodologies used in these tasks are useful resources for feature and model selections.',\n",
    "              'In the overview paper of the authorship attribution task of PAN 2019\\cite{kestemont2019overview}, the 12 best performing models are compiled. All features involve character n-grams and other sorts of n-grams. Other popular choices of n-grams contain words, POS tags, punctuation and distortion. Most of the participants use TF-IDF weighting and SVMs as classifiers. Distortion is a method for masking topical contents of the text before feature extraction, focusing only on stylometric features.']\n",
    "test_corpus = ['I have been unable to generate numbers by the Bohr hypothesis, but given more time, I would have simulated multiple distributions following the formula and compare the peak locations to my fit results, by looking at their means and if plausible, perform a t- or z-test to compare. Perhaps using a Kolmogorov-Smirnov test on the entirety of the data would show if it matches only by the peaks or by the whole distribution.',\n",
    "               \"In recent years there have been an increasing interest in biometric identification. Various studies has been conducted aiming to detect individual or unique demographic traits. Results from recent studies have given reason to believe that the ability to predict demographic information using eye tracking is highly dependent on the stimulus. Based on this, this study performs a comparison using two datasets built on different stimulus: IQ tests and passive image viewing. This study found significant differences in the accuracy achieved on the two datasets using a Random Forest (RF) and Long-Short Term Memory network (LSTM) as classifiers. The random forest performed best with an accuracy of 0.85 on the passive image viewing dataset and 0.70 on the IQ dataset. Furthermore, we investigated the effect of using micro movements of the eye as features in the model, which has proven to be a useful feature in recent studies within biometric identification. Lastly, we report the classifiers' ability to perform task independent predictions of gender, using one dataset for training and another for testing. The result from this experiments was not satisfying and close to chance level. However, combining the dataset for training and testing resulted in the random forest having an overall accuracy on the combined dataset of 0.74.\",\n",
    "              ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats for our test document\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'number_of_sentences': 2,\n",
       " 'avg_sent_len_chars': 183.0,\n",
       " 'std_sent_len_chars': 176.0,\n",
       " 'med_sent_len_chars': 183.0,\n",
       " 'avg_sent_len_words': 28.0,\n",
       " 'std_sent_len_words': 27.0,\n",
       " 'med_sent_len_words': 28.0}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_sentences(text):\n",
    "    '''Returns an array with text split into sentences'''\n",
    "    return np.array(re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', text), dtype=str)\n",
    "\n",
    "def remove_dots(word):\n",
    "    return re.sub(r',|\\.|:|!|\\?|;', '', word)\n",
    "\n",
    "def split_words(text):\n",
    "    '''Returns an array with text split into words'''\n",
    "    text = text.lower()\n",
    "    string = np.array(text.split(), dtype=str)\n",
    "    no_dot = np.array([remove_dots(x) for x in string])\n",
    "    return np.array(list(filter(None, no_dot)))\n",
    "\n",
    "def get_sentence_lengths(text):\n",
    "    '''Returns dictionary with sentence lengh in chars and words'''\n",
    "    split_text = split_sentences(text)\n",
    "    num_sentences = len(split_text)\n",
    "    num_chars = np.array([len(x) for x in split_text], dtype=int)\n",
    "    num_words = [split_words(x).size for x in split_sentences(string)]\n",
    "    return {'chars' : num_chars, 'words' : num_words, 'num_sents' : num_sentences}\n",
    "\n",
    "def get_sentence_length_stats(text):\n",
    "    '''Returns dictionary with mean, std and median lengths in both chars and words'''\n",
    "    sentence_lengths = get_sentence_lengths(text)\n",
    "    output = {'number_of_sentences' : sentence_lengths['num_sents'],\n",
    "              'avg_sent_len_chars' : np.mean(sentence_lengths['chars']),\n",
    "              'std_sent_len_chars' : np.std(sentence_lengths['chars']),\n",
    "              'med_sent_len_chars' : np.median(sentence_lengths['chars']),\n",
    "              \n",
    "              'avg_sent_len_words' : np.mean(sentence_lengths['words']),\n",
    "              'std_sent_len_words' : np.std(sentence_lengths['words']),\n",
    "              'med_sent_len_words' : np.median(sentence_lengths['words'])\n",
    "             }\n",
    "    return output\n",
    "\n",
    "print(\"Stats for our test document\")\n",
    "get_sentence_length_stats(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Word length\n",
    "The count of words of the entire text.\n",
    "Also extremely complex feature, cool shit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test word lengths\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'number_of_words': 56,\n",
       " 'avg_word_len_chars': 5.339285714285714,\n",
       " 'std_word_len_chars': 3.4500351225659682,\n",
       " 'med_word_len_chars': 4.0}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_word_lengths(split_text):\n",
    "    '''Returns length of words in characters'''\n",
    "    return np.array([len(x) for x in split_text], dtype=int)\n",
    "\n",
    "def get_word_length_stats(text):\n",
    "    '''Returns various stats for words in document'''\n",
    "    #Split text here, to reduce function calls.\n",
    "    split_text = split_words(text)\n",
    "    word_lengths = get_word_lengths(split_text)\n",
    "    output = {\n",
    "        'number_of_words' : len(split_text),\n",
    "        'avg_word_len_chars' : np.mean(word_lengths),\n",
    "        'std_word_len_chars' : np.std(word_lengths),\n",
    "        'med_word_len_chars' : np.median(word_lengths)\n",
    "    }\n",
    "    return output\n",
    "\n",
    "print(\"Test word lengths\")\n",
    "get_word_length_stats(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word frequency\n",
    "Get word frequencies with TF-IDF weightings.\n",
    "\n",
    "Word frequency is the same as word unigrams - defined later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf_names(vectorizer, X, h=5):\n",
    "    ngrams = vectorizer.get_feature_names_out()\n",
    "    dense = X.todense()\n",
    "    denselist = dense.tolist()\n",
    "    df = pd.DataFrame(denselist, columns=ngrams)\n",
    "    display(df.head(h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hapax legomena\n",
    "Count how many unique words are in a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_hapax(split_text):\n",
    "    '''Returns the numer of hapax legomena, takes a split words as input'''\n",
    "    fdist = nltk.probability.FreqDist(split_text)\n",
    "    return len(fdist.hapaxes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexical diversity\n",
    "Compute single-values describing the lexical diversities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_lix(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    splt = text.split()\n",
    "    o = len(splt)+1\n",
    "    p = len([x for x in tokens if x=='.'])+1\n",
    "    l = len([x for x in tokens if len(x)>6])+1\n",
    "    \n",
    "    return (o/p)+((l*100)/o)\n",
    "\n",
    "def lexical_diversities(text):\n",
    "    '''Returns a dictionary with various vocabulary richness measures'''\n",
    "    lex = LexicalRichness(text, tokenizer=split_words)\n",
    "    try:\n",
    "        dugast = lex.Dugast\n",
    "    except:\n",
    "        dugast = 0\n",
    "    \n",
    "    try:\n",
    "        herdan = lex.Herdan\n",
    "    except:\n",
    "        herdan = 0\n",
    "    \n",
    "    try:\n",
    "        summer = lex.Summer\n",
    "    except:\n",
    "        summer = 0\n",
    "    \n",
    "    try:\n",
    "        maas = lex.Maas\n",
    "    except:\n",
    "        maas = 0\n",
    "    \n",
    "    try:\n",
    "        ttr = lex.ttr\n",
    "    except:\n",
    "        ttr = 0\n",
    "    \n",
    "    try:\n",
    "        rttr = lex.rttr\n",
    "    except:\n",
    "        rttr = 0\n",
    "    \n",
    "    try:\n",
    "        cttr = lex.cttr\n",
    "    except:\n",
    "        cttr = 0\n",
    "        \n",
    "    output = {\n",
    "        'ttr' : ttr,\n",
    "        'rttr' : rttr,\n",
    "        'cttr' : cttr,\n",
    "        'herdan' : herdan,\n",
    "        'summer' : summer,\n",
    "        'dugast' : dugast,\n",
    "        'maas' : maas,\n",
    "        'lix' : compute_lix(text)\n",
    "    }\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1673b339188>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "stats = [lexical_diversities(x) for x in train_corpus]\n",
    "\n",
    "ttr = [x['ttr'] for x in stats]\n",
    "rttr = [x['rttr'] for x in stats]\n",
    "cttr = [x['cttr'] for x in stats]\n",
    "herdan = [x['herdan'] for x in stats]\n",
    "summer = [x['summer'] for x in stats]\n",
    "maas = [x['maas'] for x in stats]\n",
    "lix = [x['lix'] for x in stats]\n",
    "\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "plt.plot(ttr, label='ttr')\n",
    "plt.plot(rttr, label='rttr')\n",
    "plt.plot(cttr, label='cttr')\n",
    "plt.plot(herdan, label='herdan')\n",
    "plt.plot(summer, label='summer')\n",
    "plt.plot(maas, label='maas')\n",
    "plt.plot(lix, label='lix')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA Topic\n",
    "Generate topics using LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.ldamulticore import LdaModel\n",
    "\n",
    "with open('function_words/clean_function_words.txt', 'r', encoding='utf-8') as f:\n",
    "    func_words = f.readlines()\n",
    "    func_words = [x.replace('\\n','') for x in func_words]\n",
    "\n",
    "def remove_symbols(text):\n",
    "    return re.sub('\\W+',' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_docs_for_lda(corpus, verbose=True):\n",
    "    if verbose:\n",
    "        docs = [remove_symbols(x) for x in tqdm(corpus)] # Remove symbols\n",
    "        docs = [split_words(x.lower().strip()) for x in tqdm(docs)] #Word tokenize all text\n",
    "        docs = [[token for token in doc if not token.isnumeric()] for doc in tqdm(docs)] #Remove numbers\n",
    "        docs = [[token for token in doc if len(token)>1] for doc in tqdm(docs)] #Remove words of 1 character\n",
    "        docs = [[token for token in doc if token not in func_words] for doc in tqdm(docs)] #Filter stop words\n",
    "    else:\n",
    "        docs = [remove_symbols(x) for x in corpus] # Remove symbols\n",
    "        docs = [split_words(x.lower().strip()) for x in docs] #Word tokenize all text\n",
    "        docs = [[token for token in doc if not token.isnumeric()] for doc in docs] #Remove numbers\n",
    "        docs = [[token for token in doc if len(token)>1] for doc in docs] #Remove words of 1 character\n",
    "        docs = [[token for token in doc if token not in func_words] for doc in docs] #Filter stop words\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def create_lda_dictionary(docs):\n",
    "    dictionary = Dictionary(docs)\n",
    "    #Remove words that occur in less than 20 documents and more than 10% of all documents\n",
    "    dictionary.filter_extremes(no_below=20, no_above=0.1)\n",
    "    print(\"Creating dictionary...\")\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in tqdm(docs)]\n",
    "    print(\"Number of unique tokens:\", len(dictionary))\n",
    "    print(\"Number of documents:\", len(corpus))\n",
    "    return dictionary, corpus\n",
    "\n",
    "def create_lda_model(docs, num_topics, epochs, save=False, load_path=None, verbose=False):\n",
    "    '''Returns LDA model and dictionary'''\n",
    "        \n",
    "    #Get data (only for file name if saved)\n",
    "    date = [x for x in time.localtime()]\n",
    "    date = str(date[2])+'/'+str(date[1])+'/'+str(date[0])\n",
    "\n",
    "    \n",
    "    #Tokenize corpus, if raw text is given.\n",
    "    if type(docs[0]) == str:\n",
    "        print(\"You provided raw text. Tokenizing first...\")\n",
    "        docs = tokenize_docs_for_lda(docs, verbose=verbose)\n",
    "        print(\"Tokenized!\")\n",
    "    \n",
    "    #Create dictionary\n",
    "    dictionary, corpus = create_lda_dictionary(docs)\n",
    "    assert len(dictionary) > 0, \"Oh no! No unique words... You need more text!\"\n",
    "    \n",
    "    if load_path:\n",
    "        print(\"Loading LDA model\", load_path)\n",
    "        return models.ldamodel.LdaModel.load(load_path), dictionary\n",
    "    \n",
    "    num_topics = num_topics\n",
    "    chunksize = 2000 #2k\n",
    "    passes = epochs #(epochs)\n",
    "    iterations = 300 #400\n",
    "    eval_every = 2\n",
    "    \n",
    "    _ = dictionary[0] #Initializes some values, necessary for model...\n",
    "    id2word = dictionary.id2token\n",
    "    model_name = \"LDA\"+date\n",
    "    model = LdaModel(corpus=corpus,\n",
    "                    id2word=id2word,\n",
    "                    chunksize=chunksize,\n",
    "                    alpha='auto',\n",
    "                    eta='auto',\n",
    "                    iterations=iterations,\n",
    "                    num_topics=num_topics,\n",
    "                    passes=passes,\n",
    "                    minimum_probability=0.0,\n",
    "                    eval_every=eval_every)\n",
    "    \n",
    "    if save:\n",
    "        model.save('data/additional/LDAmodel/'+model_name)\n",
    "    \n",
    "    topics = [[', '.join([x[0] for x in model.show_topic(topic)])] for topic in range(model.num_topics)]\n",
    "    for i in range(len(topics)):\n",
    "        print(f\"Topic {i}:\", topics[i][0])\n",
    "    \n",
    "    return model, dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_topic(article, dictionary, model):\n",
    "    '''Returns the list of possible topics, sorted by score'''\n",
    "    if type(article)==str:\n",
    "        article = tokenize_docs_for_lda([article], verbose=False)[0]\n",
    "    \n",
    "    doc_bow = dictionary.doc2bow(article)\n",
    "    topics = model[doc_bow]\n",
    "    return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 2., 3., 4.])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics[:,:,0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = np.array([predict_topic(x, lda_vars[1], lda_vars[0]) for x in ['yolo']*100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['topic 0', 'topic 1', 'topic 2', 'topic 3', 'topic 4']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[f'topic {int(i)}' for i in topics[:,:,0][0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function words\n",
    "Computing TF-IDF of documents when they're represented as only the function words that appear in given document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_function_words_vectorizer(trainset, function_words):\n",
    "    \n",
    "    fw_corpus = []\n",
    "    \n",
    "    for document in tqdm(trainset): \n",
    "        fw_document = \"\"\n",
    "        document = remove_symbols(document)\n",
    "        words = document.split() #split text into words\n",
    "        \n",
    "        for word in words: \n",
    "            if word in function_words:\n",
    "                fw_document = fw_document + word + \" \" #concatenate all function words in doc as doc representation\n",
    "        \n",
    "        fw_corpus.append(fw_document)\n",
    "        \n",
    "    \n",
    "    vectorizer = TfidfVectorizer(max_features=1000, analyzer=\"word\")\n",
    "    print(\"Training FW vectorizer...\",rand_emot())\n",
    "    \n",
    "    X = vectorizer.fit_transform(fw_corpus)\n",
    "    print(\"Vectorizer fit!\", rand_emot())\n",
    "    \n",
    "    return X, vectorizer#, fw_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Grams\n",
    "A collection of vectorizers for different types of n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import skipgrams\n",
    "\n",
    "def skipgram_vectorizer(train_corpus, n, k): #k = step (skip) size\n",
    "    \n",
    "    skipgram_corpus = []\n",
    "    \n",
    "    for doc in train_corpus: \n",
    "        l = list(skipgrams(doc.split(), n, k))\n",
    "        new_doc = ' '.join([' '.join(x) for x in l]) #concatenate skipgrams as new representation of doc\n",
    "        skipgram_corpus.append(new_doc)\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(max_features=1000, analyzer=\"word\", ngram_range=(n,n))\n",
    "    print(f\"Training skipgram (k={k}) vectorizer...\",rand_emot())\n",
    "    \n",
    "    X = vectorizer.fit_transform(skipgram_corpus)\n",
    "    print(\"Vectorizer fit!\", rand_emot())\n",
    "    \n",
    "    ngrams = vectorizer.get_feature_names_out()\n",
    "      \n",
    "    return X, vectorizer #, skipgram_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_tagger_DK = spacy.load(\"da_core_news_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_ngram_vectorizer(train_corpus, n): \n",
    "    #Trains a TF-IDF vectorizer of word n-grams\n",
    "    vectorizer = TfidfVectorizer(max_features=1000, analyzer=\"word\", ngram_range=(n,n))\n",
    "    print(f\"Training word {n}-gram vectorizer...\",rand_emot())\n",
    "    \n",
    "    X = vectorizer.fit_transform(train_corpus)\n",
    "    print(\"Vectorizer fit!\", rand_emot())\n",
    "    \n",
    "    return X, vectorizer\n",
    "\n",
    "def char_ngram_vectorizer(train_corpus, n): \n",
    "    #Trains a TF-IDF vectorizer of character n-grams\n",
    "    vectorizer = TfidfVectorizer(max_features=1000, analyzer=\"char\", ngram_range=(n,n))\n",
    "    print(f\"Training char {n}-gram vectorizer...\",rand_emot())\n",
    "    \n",
    "    X = vectorizer.fit_transform(train_corpus)\n",
    "    print(\"Vectorizer fit!\", rand_emot())\n",
    "      \n",
    "    return X, vectorizer\n",
    "\n",
    "def extract_POS(corpus, load_corpus=None):\n",
    "    POS_corpus = []\n",
    "    if load_corpus:\n",
    "        assert type(load_corpus) == str, \"load_corpus should be a file path to POS-tagget corpus\"\n",
    "        print(\"Loading POS-tagged corpus...\")\n",
    "        with open(load_corpus, 'rb') as f:\n",
    "            POS_corpus = pickle.load(f)\n",
    "            print(\"Loaded!\", rand_emot())\n",
    "        return POS_corpus\n",
    "        \n",
    "    print(\"Extracting POS...\")\n",
    "    \n",
    "    for doc in tqdm(corpus):\n",
    "        tagged_doc = POS_tagger_DK(doc) #tag each document in corpus with POS tags using spacy\n",
    "        POS_list = []\n",
    "\n",
    "        for token in tagged_doc:\n",
    "            POS_list.append(token.pos_)\n",
    "\n",
    "        #concatenate as POS tags for the document\n",
    "        POS_text = \" \".join(POS_list)\n",
    "        POS_corpus.append(POS_text)\n",
    "    return POS_corpus\n",
    "\n",
    "def POS_ngram_vectorizer(POS_corpus, n): \n",
    "    #Trains a TF-IDF vectorizer of POS n-grams. A POS corpus is generated in the function using a tagger for Danish.\n",
    "   \n",
    "    vectorizer = TfidfVectorizer(max_features=1000, analyzer=\"word\", ngram_range=(n,n))\n",
    "    print(f\"Training POS {n}-gram vectorizer...\",rand_emot())\n",
    "    \n",
    "    X = vectorizer.fit_transform(POS_corpus)\n",
    "    print(\"Vectorizer fit!\", rand_emot())\n",
    "\n",
    "    return X, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets weights for terms based on trained vectorizer\n",
    "# Works for both word and character ngrams\n",
    "def get_tfidf_ngrams(vectorizer, test_corpus):\n",
    "    '''Returns the TF-IDF weighted ngram frequencies of test documents'''\n",
    "    #Multiple texts required\n",
    "    return vectorizer.transform(test_corpus)\n",
    "\n",
    "# Function generates POS test corpus first and then gets weights for terms based on trained vectorizer. \n",
    "def get_tfidf_POS_ngrams(vectorizer, POS_corpus):\n",
    "    '''Returns the TF-IDF weighted ngram frequencies of test documents'''    \n",
    "    #Multiple texts required\n",
    "    return vectorizer.transform(POS_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf_function_words(vectorizer, testset):\n",
    "    \n",
    "    fw_corpus = []\n",
    "    \n",
    "    for document in tqdm(testset): \n",
    "        fw_document = \"\"\n",
    "        document = remove_symbols(document)\n",
    "        words = document.split() #split text into words\n",
    "        \n",
    "        for word in words: \n",
    "            if word in func_words: #if word is a function word\n",
    "                fw_document = fw_document + word + \" \" #add word to fw_document - doc represented as the function words that\n",
    "                #appear in it\n",
    "        \n",
    "        fw_corpus.append(fw_document)\n",
    "        \n",
    "    fw_vectorized = get_tfidf_ngrams(vectorizer, fw_corpus)\n",
    "    \n",
    "    return fw_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "def train_vectorizers(training_data, POS_corpus_path=None, save=False):\n",
    "    '''Returns 2 dictionaries: vectorizers and weights\n",
    "    Only use this function on training data'''\n",
    "    \n",
    "    #Word n-gram vectorizers\n",
    "    X_word_unigrams, unigram_word_vectorizer = word_ngram_vectorizer(training_data, 1)\n",
    "    X_word_bigrams, bigram_word_vectorizer = word_ngram_vectorizer(training_data, 2)\n",
    "    X_word_trigrams, trigram_word_vectorizer = word_ngram_vectorizer(training_data, 3)\n",
    "    \n",
    "    #Character n-gram vectorizers\n",
    "    X_char_unigrams, unigram_char_vectorizer = char_ngram_vectorizer(training_data, 1)\n",
    "    X_char_bigrams, bigram_char_vectorizer = char_ngram_vectorizer(training_data, 2)\n",
    "    X_char_trigrams, trigram_char_vectorizer = char_ngram_vectorizer(training_data, 3)\n",
    "    \n",
    "    POS_corpus = extract_POS(training_data, load_corpus=POS_corpus_path)\n",
    "    \n",
    "    if save and not POS_corpus_path:\n",
    "        assert POS_corpus_path, \"Why the FUCK do you tell me to save without telling me where? Give med POS corpus path!\"\n",
    "        with open(POS_corpus_path, 'wb') as f:\n",
    "            pickle.dump(POS_corpus, f)\n",
    "            print(\"POS corpus saved.\")\n",
    "            \n",
    "    #POS n-gram vectorizers\n",
    "    X_POS_unigrams, unigram_POS_vectorizer = POS_ngram_vectorizer(POS_corpus, 1)\n",
    "    X_POS_bigrams, bigram_POS_vectorizer = POS_ngram_vectorizer(POS_corpus, 2)\n",
    "    X_POS_trigrams, trigram_POS_vectorizer = POS_ngram_vectorizer(POS_corpus, 3)\n",
    "    \n",
    "    #Skip-gram vectorizers\n",
    "    X_skip_1_bigram, skip_1_vectorizer = skipgram_vectorizer(training_data, 2, 1)  \n",
    "    X_skip_2_bigram, skip_2_vectorizer = skipgram_vectorizer(training_data, 2, 2)\n",
    "    X_skip_3_bigram, skip_3_vectorizer = skipgram_vectorizer(training_data, 2, 3)\n",
    "    \n",
    "    #Function word vectorizer\n",
    "    X_function_words, function_words_vectorizer = train_function_words_vectorizer(training_data, func_words)\n",
    "    \n",
    "    \n",
    "    \n",
    "    vectorizers = {\n",
    "        'uni_word' : unigram_word_vectorizer,\n",
    "        'bi_word' : bigram_word_vectorizer,\n",
    "        'tri_word' : trigram_word_vectorizer,\n",
    "        'uni_char' : unigram_char_vectorizer,\n",
    "        'bi_char' : bigram_char_vectorizer,\n",
    "        'tri_char' : trigram_char_vectorizer,\n",
    "        'uni_pos' : unigram_POS_vectorizer,\n",
    "        'bi_pos' : bigram_POS_vectorizer,\n",
    "        'tri_pos' : trigram_POS_vectorizer,\n",
    "        'bi_skip1': skip_1_vectorizer, \n",
    "        'bi_skip2': skip_2_vectorizer, \n",
    "        'bi_skip3': skip_3_vectorizer,\n",
    "        'function_words': function_words_vectorizer\n",
    "    }\n",
    "    \n",
    "    weights = {\n",
    "        'uni_word' : X_word_unigrams,\n",
    "        'bi_word' : X_word_bigrams,\n",
    "        'tri_word' : X_word_trigrams,\n",
    "        'uni_char' : X_char_unigrams,\n",
    "        'bi_char' : X_char_bigrams,\n",
    "        'tri_char' : X_char_trigrams,\n",
    "        'uni_pos' : X_POS_unigrams,\n",
    "        'bi_pos' : X_POS_bigrams,\n",
    "        'tri_pos' : X_POS_trigrams,\n",
    "        'bi_skip1': X_skip_1_bigram, \n",
    "        'bi_skip2': X_skip_2_bigram, \n",
    "        'bi_skip3': X_skip_3_bigram,\n",
    "        'function_words': X_function_words\n",
    "    }\n",
    "    \n",
    "    return vectorizers, weights\n",
    "\n",
    "vectorizers, weights = train_vectorizers(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>12</th>\n",
       "      <th>2019</th>\n",
       "      <th>about</th>\n",
       "      <th>al</th>\n",
       "      <th>all</th>\n",
       "      <th>along</th>\n",
       "      <th>and</th>\n",
       "      <th>are</th>\n",
       "      <th>argue</th>\n",
       "      <th>article</th>\n",
       "      <th>...</th>\n",
       "      <th>were</th>\n",
       "      <th>when</th>\n",
       "      <th>which</th>\n",
       "      <th>whole</th>\n",
       "      <th>will</th>\n",
       "      <th>with</th>\n",
       "      <th>word</th>\n",
       "      <th>words</th>\n",
       "      <th>writing</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.091857</td>\n",
       "      <td>0.074110</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.091857</td>\n",
       "      <td>0.043770</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.091857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.183714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.061518</td>\n",
       "      <td>0.183714</td>\n",
       "      <td>0.222329</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.091857</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083392</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.197010</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.206724</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.103362</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.276891</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.126477</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.074699</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.156765</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052494</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.063239</td>\n",
       "      <td>0.052494</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.194620</td>\n",
       "      <td>0.109840</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136144</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.091177</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.136144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.120723</td>\n",
       "      <td>0.097399</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.120723</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.172576</td>\n",
       "      <td>0.097399</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080850</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 197 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         12      2019     about        al       all     along       and  \\\n",
       "0  0.000000  0.000000  0.091857  0.074110  0.000000  0.091857  0.043770   \n",
       "1  0.000000  0.000000  0.000000  0.083392  0.000000  0.000000  0.197010   \n",
       "2  0.000000  0.126477  0.000000  0.000000  0.000000  0.000000  0.074699   \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.194620   \n",
       "4  0.120723  0.097399  0.000000  0.000000  0.120723  0.000000  0.172576   \n",
       "\n",
       "        are     argue   article  ...      were      when     which     whole  \\\n",
       "0  0.000000  0.091857  0.000000  ...  0.000000  0.183714  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.206724  ...  0.000000  0.000000  0.000000  0.103362   \n",
       "2  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.156765  0.000000   \n",
       "3  0.109840  0.000000  0.000000  ...  0.136144  0.000000  0.000000  0.000000   \n",
       "4  0.097399  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "       will      with      word     words   writing      year  \n",
       "0  0.061518  0.183714  0.222329  0.000000  0.091857  0.000000  \n",
       "1  0.000000  0.000000  0.000000  0.276891  0.000000  0.000000  \n",
       "2  0.052494  0.000000  0.063239  0.052494  0.000000  0.000000  \n",
       "3  0.091177  0.000000  0.000000  0.000000  0.000000  0.136144  \n",
       "4  0.000000  0.000000  0.000000  0.080850  0.000000  0.000000  \n",
       "\n",
       "[5 rows x 197 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_tfidf_names(vectorizers['uni_word'], weights['uni_word'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining features\n",
    "Combine the features into one long as fuck vector!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_test_data(vectorizers, test_data, POS_corpus_path=None):\n",
    "    '''Returns weighted matrices for test data.'''\n",
    "    POS_corpus = extract_POS(test_data, load_corpus=POS_corpus_path)\n",
    "    weights = {}\n",
    "    for vectorizer in tqdm(vectorizers):\n",
    "        if 'pos' in vectorizer:\n",
    "            weights[vectorizer] = get_tfidf_POS_ngrams(vectorizers[vectorizer], POS_corpus)\n",
    "        if vectorizer == 'function_words': \n",
    "            weights[vectorizer] = get_tfidf_function_words(vectorizers[vectorizer], test_data)\n",
    "        else:\n",
    "            weights[vectorizer] = get_tfidf_ngrams(vectorizers[vectorizer], test_data)\n",
    "    return weights\n",
    "\n",
    "def combine_data(data, train_test, vectorizers=None, POS_corpus_path=None, save=False, LDA_path=None, LDA_model=None, LDA_dict=None):\n",
    "    topics = 5\n",
    "    lda_epochs = 1\n",
    "    #Trains vectorizers if training data is specified.\n",
    "    if train_test == 'train':\n",
    "        assert vectorizers == None, \"Please do not specify vectorizers when training them!\"\n",
    "        print(\"Training data specified... Training vectorizers!\\n\")\n",
    "        vectorizers, weights = train_vectorizers(data, POS_corpus_path=POS_corpus_path, save=save)\n",
    "        lda_model, lda_dictionary = create_lda_model(data, topics, lda_epochs, save=save, load_path=LDA_path)\n",
    "        \n",
    "    elif train_test == 'test':\n",
    "        assert type(vectorizers) == dict, \"Vectorizers need to be in a dict!\"\n",
    "        assert LDA_model, \"I want a trained LDA model passed as LDA_model for test data!\"\n",
    "        assert LDA_dict, \"I fucking want a LDA dictionary! I don't know what to do with this LDA model without a dictionary!! ... Sorry...\"\n",
    "        print(\"Test data being vectorized...\")\n",
    "        weights = vectorize_test_data(vectorizers, data)\n",
    "        lda_model, lda_dictionary = LDA_model, LDA_dict\n",
    "        \n",
    "    elif train_test == 'load':\n",
    "        assert type(vectorizers) == dict, \"Don't load if you don't have any vectoriers!\"\n",
    "        print(\"You have decided to use loaded vectorizers! How daring...\")\n",
    "        weights = vectorize_test_data(vectorizers, data, POS_corpus_path=POS_corpus_path)\n",
    "        lda_model, lda_dictionary = create_lda_model(data, topics, lda_epochs, save=save, load_path=LDA_path)\n",
    "    else:\n",
    "        print(\"Please specify type of data!\")\n",
    "        return\n",
    "    \n",
    "    if save and train_test == 'train':\n",
    "        with open('data/backup/vectorizers.dat', 'wb') as f:\n",
    "            pickle.dump(vectorizers, f)\n",
    "        with open('data/backup/weights_train.dat', 'wb') as f:\n",
    "            pickle.dump(vectorizers, f)\n",
    "    elif save and train_test == 'test':\n",
    "        with open('data/backup/weights_test.dat', 'wb') as f:\n",
    "            pickle.dump(vectorizers, f)\n",
    "    \n",
    "    feature_indices = []\n",
    "    \n",
    "    topics = np.array([predict_topic(x, lda_dictionary, lda_model) for x in data])\n",
    "    \n",
    "    uni_word = weights['uni_word']\n",
    "    bi_word = weights['bi_word']\n",
    "    tri_word = weights['tri_word']\n",
    "    \n",
    "    uni_char = weights['uni_char']\n",
    "    bi_char = weights['bi_char']\n",
    "    tri_char = weights['tri_char']\n",
    "    \n",
    "    uni_pos = weights['uni_pos']\n",
    "    bi_pos = weights['bi_pos']\n",
    "    tri_pos = weights['tri_pos']\n",
    "    \n",
    "    bi_skip1 = weights['bi_skip1']\n",
    "    bi_skip2 = weights['bi_skip2']\n",
    "    bi_skip3 = weights['bi_skip3']\n",
    "    \n",
    "    tfidf_func = weights['function_words']\n",
    "    \n",
    "    #Stack ngrams\n",
    "    word_ngrams = scipy.sparse.hstack((uni_word, bi_word, tri_word))\n",
    "    char_ngrams = scipy.sparse.hstack((uni_char, bi_char, tri_char))\n",
    "    pos_ngrams = scipy.sparse.hstack((uni_pos, bi_pos, tri_pos))\n",
    "    skip_ngrams = scipy.sparse.hstack((bi_skip1, bi_skip2, bi_skip3))\n",
    "    ngrams = scipy.sparse.hstack((word_ngrams, char_ngrams, pos_ngrams, skip_ngrams, tfidf_func))\n",
    "    print()\n",
    "    print(\"N-grams shape:\", ngrams.shape) \n",
    "    \n",
    "    print(\"TOPICS\")\n",
    "    print(topics.shape)\n",
    "    #Stack scalar features\n",
    "    sentence_stats = [list(get_sentence_length_stats(x).values()) for x in tqdm(data)]\n",
    "    word_stats = [list(get_word_length_stats(x).values()) for x in tqdm(data)]\n",
    "    lexical_diversity = [list(lexical_diversities(x).values()) for x in tqdm(data)]\n",
    "    hapax = [[get_num_hapax(x)] for x in data]\n",
    "    scalars = np.hstack((sentence_stats, word_stats, lexical_diversity,hapax,topics[:,:,1])) #I think the topic is appended correctly\n",
    "    print(\"Scalars shape:\", scalars.shape)\n",
    "    \n",
    "    #Add indices to feature names\n",
    "    [feature_indices.append('uni_word_ngram') for i in range(uni_word.shape[-1])]\n",
    "    [feature_indices.append('bi_word_ngram') for i in range(bi_word.shape[-1])]\n",
    "    [feature_indices.append('tri_word_ngram') for i in range(tri_word.shape[-1])]\n",
    "    [feature_indices.append('uni_char_ngram') for i in range(uni_char.shape[-1])]\n",
    "    [feature_indices.append('bi_char_ngram') for i in range(bi_char.shape[-1])]\n",
    "    [feature_indices.append('tri_char_ngram') for i in range(tri_char.shape[-1])]\n",
    "    [feature_indices.append('uni_pos_ngram') for i in range(uni_pos.shape[-1])]\n",
    "    [feature_indices.append('bi_pos_ngram') for i in range(bi_pos.shape[-1])]\n",
    "    [feature_indices.append('tri_pos_ngram') for i in range(tri_pos.shape[-1])]\n",
    "    [feature_indices.append('bi_1_skipgram') for i in range(bi_skip1.shape[-1])]\n",
    "    [feature_indices.append('bi_2_skipgram') for i in range(bi_skip2.shape[-1])]\n",
    "    [feature_indices.append('bi_3_skipgram') for i in range(bi_skip3.shape[-1])]\n",
    "    [feature_indices.append('tfidf_func_words') for i in range(tfidf_func.shape[-1])]\n",
    "    \n",
    "    #Scalar features\n",
    "    [feature_indices.append(i) for i in get_sentence_length_stats(data[0]).keys()]\n",
    "    [feature_indices.append(i) for i in get_word_length_stats(data[0]).keys()]\n",
    "    [feature_indices.append(i) for i in lexical_diversities(data[0]).keys()]\n",
    "    feature_indices.append('num_hapax')\n",
    "    [feature_indices.append(f'topic {int(i)}') for i in topics[:,:,0][0]]\n",
    "    \n",
    "    return scipy.sparse.hstack((ngrams, scalars)), vectorizers, np.array(feature_indices, dtype=str), (lda_model, lda_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract the features\n",
    "Extract the features from all the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "281c44a5e94f43f491f53cd4af4edd4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path = 'data/additional/scraped'\n",
    "files = os.listdir(path)\n",
    "dataframes = []\n",
    "for file in tqdm(files):\n",
    "    dataframes.append(pd.read_json(path+'/'+file))\n",
    "\n",
    "dataframe = pd.concat(dataframes)\n",
    "del dataframe['level_0']\n",
    "del dataframe['index']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split into train/test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('data/backup/vectorizers.dat', 'rb') as f:\n",
    "#     vectorizers = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data specified... Training vectorizers!\n",
      "\n",
      "Training word 1-gram vectorizer... x)\n",
      "Vectorizer fit! ( ◡́.◡̀)\n",
      "Training word 2-gram vectorizer... (ㆆ_ㆆ)\n",
      "Vectorizer fit! (ㆆ_ㆆ)\n",
      "Training word 3-gram vectorizer... (̶◉͛‿◉̶)\n",
      "Vectorizer fit! :-)\n",
      "Training char 1-gram vectorizer... ʕ•́ᴥ•̀ʔっ\n",
      "Vectorizer fit! ( ◡́.◡̀)\n",
      "Training char 2-gram vectorizer... :-)\n",
      "Vectorizer fit! (o_o)\n",
      "Training char 3-gram vectorizer... *<:-)\n",
      "Vectorizer fit! *<:-)\n",
      "Extracting POS...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25cafd30347a4c518ecffba17b6ae00d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training POS 1-gram vectorizer... :P\n",
      "Vectorizer fit! ʕ•́ᴥ•̀ʔっ\n",
      "Training POS 2-gram vectorizer... (o_o)\n",
      "Vectorizer fit! ╯°□°）╯︵ ┻━┻\n",
      "Training POS 3-gram vectorizer... ^_^\n",
      "Vectorizer fit! :D\n",
      "Training skipgram (k=1) vectorizer... (ㆆ_ㆆ)\n",
      "Vectorizer fit! (o_o)\n",
      "Training skipgram (k=2) vectorizer... :-)\n",
      "Vectorizer fit! :D\n",
      "Training skipgram (k=3) vectorizer... ¯\\_(ツ)_/¯\n",
      "Vectorizer fit! ( ͡❛ ͜ʖ ͡❛)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6120b31cead940658e0bc525de4c83f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training FW vectorizer... ╯°□°）╯︵ ┻━┻\n",
      "Vectorizer fit! ( ͡❛ ͜ʖ ͡❛)\n",
      "You provided raw text. Tokenizing first...\n",
      "Tokenized!\n",
      "Creating dictionary...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6077d949fff74a858fe158233d5fe925",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 178\n",
      "Number of documents: 400\n",
      "Topic 0: bedste, københavn, tilmelding, mennesker, unge, nyhedsbrev, anbefalinger, fredag, fire, små\n",
      "Topic 1: dansk, årige, køber, undersøgelse, ville, lave, varer, stadig, tidligere, forbrugere\n",
      "Topic 2: tv, unge, fik, forældrene, gælder, folk, forskellige, fået, svært, let\n",
      "Topic 3: læs, dk, samme, grønne, skriver, igen, følg, ligger, mor, par\n",
      "Topic 4: tøj, bruger, bedre, typisk, penge, lave, køber, altid, børnene, abonnement\n",
      "\n",
      "N-grams shape: (400, 9448)\n",
      "TOPICS\n",
      "(400, 5, 2)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78d4f6e3b9db48ef93ae3a8d0b23deec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edf2f671afc54be7aa6171a05ccd1a1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e61f27a5edba4f0a8b8d7c7c98827929",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scalars shape: (400, 25)\n",
      "Training data done!\n",
      "--------------------------------------------------\n",
      "\n",
      "Test data being vectorized...\n",
      "Extracting POS...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b267d4d18857442a91861f0f3c0e673f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11642184f993400aae16cdfa0f866b1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1894bb0bbae4fdfa583569c2ca6156b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "N-grams shape: (100, 9448)\n",
      "TOPICS\n",
      "(100, 5, 2)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b77d48406674a57adeb463a408f3a68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4334b00da1bd41ee826ea530f35d6e86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a95951876854b30b134d6abca682c33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scalars shape: (100, 25)\n",
      "Train shape: (400, 9473)\n",
      "Test shape: (100, 9473)\n",
      "Wall time: 23.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "extract = True\n",
    "if extract:\n",
    "    bodies = list(dataframe[\"Body\"])[:500]\n",
    "    authors = list(dataframe[\"Byline\"])[:500]\n",
    "\n",
    "    train_X, test_X, train_y, test_y = train_test_split(bodies, authors, test_size=0.2, random_state=42, stratify=authors)\n",
    "\n",
    "    #Convert names to categories\n",
    "    le = LabelEncoder()\n",
    "    train_y = le.fit_transform(train_y)\n",
    "    test_y = le.fit_transform(test_y)\n",
    "\n",
    "    train_X, vectorizers, feature_names, lda_vars = combine_data(train_X, 'train', save=False)#, POS_corpus_path='data/backup/POS_corpus.dat')#), vectorizers=vectorizers)#, POS_corpus_path='data/backup/POS_corpus.dat') #LOADING, NOT TRAINING\n",
    "    print(\"Training data done!\")\n",
    "    print(\"-\"*50)\n",
    "    print()\n",
    "    \n",
    "    test_X, _, _, lda_test_vars = combine_data(test_X, 'test', vectorizers=vectorizers, LDA_model=lda_vars[0], LDA_dict=lda_vars[1])\n",
    "    \n",
    "    print(\"Train shape:\", train_X.shape)\n",
    "    print(\"Test shape:\", test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feat_path = 'data/additional/features/'\n",
    "# np.save(feat_path+'trainX.npz', train_X)\n",
    "# np.save(feat_path+'trainY.npz', train_y)\n",
    "# np.save(feat_path+'testX.npz', test_X)\n",
    "# np.save(feat_path+'testY.npz', test_y)\n",
    "# np.save(feat_path+'feature_names.npz', feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X.data = np.nan_to_num(train_X.data) #Remove NaN and Inf\n",
    "test_X.data = np.nan_to_num(test_X.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9256410256410257"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import preprocessing\n",
    "\n",
    "lrg = LogisticRegression(max_iter=100, n_jobs=2)\n",
    "lrg.fit(train_X, train_y)\n",
    "f1_score(test_y, lrg.predict(test_X), average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh, model already fit. Thanks!\n",
      "Training dummy...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'bool' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Skole\\Speciale\\articlencoding\\classifier_unit_test.py\u001b[0m in \u001b[0;36mtest_classifier\u001b[1;34m(classifier, X_train, X_test, y_train, y_test, strategy, give_roc, give_importance)\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[0mdum_f1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdum_preds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'weighted'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[0mauc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreds_prob\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[0mdum_auc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdum_probs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'bool' has no len()"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import classifier_unit_test\n",
    "output = classifier_unit_test.test_classifier(lrg, train_X, test_X, train_y, test_y, give_roc=True, give_importance=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13504\\1515267637.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtpr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tpr'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mfpr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'fpr'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdum_tpr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdum_fpr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'dum_tpr'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'dum_fpr'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfpr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtpr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdum_fpr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdum_tpr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'output' is not defined"
     ]
    }
   ],
   "source": [
    "tpr = output['tpr']\n",
    "fpr = output['fpr']\n",
    "dum_tpr, dum_fpr = output['dum_tpr'], output['dum_fpr']\n",
    "plt.plot(fpr,tpr)\n",
    "plt.plot(dum_fpr, dum_tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sorted(output['importances_mean'])[::-1][-20:], label=\"Top 20 features\")\n",
    "plt.plot(sorted(output['importances_mean'])[::-1][:20], label=\"Bottom 20 features\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
