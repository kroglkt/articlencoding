{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import spacy \n",
    "import json\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_emot():\n",
    "    e = [\"(o_o)\",\":-)\",\":P\",\":D\",\"x)\",\"ᓚᘏᗢ\",\"╯°□°）╯︵ ┻━┻\",\":)\",\n",
    "         \"*<:-)\",\"^_^\",\"(⌐■_■)\",\"¯\\_(ツ)_/¯\", \"(T_T)\",\":o\",\"OwO\",\n",
    "        \"( ͡❛ ͜ʖ ͡❛)\",\"(̶◉͛‿◉̶)\",\"( ≖.≖)\",\"(ㆆ_ㆆ)\",\"ʕ•́ᴥ•̀ʔっ\",\"( ◡́.◡̀)\",\"(^◡^ )\"]\n",
    "    return random.choice(e)\n",
    "\n",
    "with open('scraped/data_30.json') as f:\n",
    "    dataframe = pd.read_json(f)\n",
    "    \n",
    "train1 = dataframe.loc[(dataframe[\"Domain\"] == \"politiken.dk\")]\n",
    "train2 = dataframe.loc[(dataframe[\"Domain\"] == \"sn.dk\")]\n",
    "train3 = dataframe.loc[(dataframe[\"Domain\"] == \"bt.dk\")]\n",
    "train4 = dataframe.loc[(dataframe[\"Domain\"] == \"version2.dk\")]\n",
    "train5 = dataframe.loc[(dataframe[\"Domain\"] == \"kommunen.dk\")]\n",
    "\n",
    "\n",
    "frames = [train1, train2,train3, train4, train5]\n",
    "trainset = pd.concat(frames, sort=False)\n",
    "bodies = list(trainset[\"Body\"])\n",
    "domains = list(trainset[\"Domain\"])\n",
    "\n",
    "train, test, train_labels, test_labels = train_test_split(bodies, domains, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_bodies = list(dataframe[\"Body\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DIN SPORTSDAG Topbrag i 1. division '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset_1 = trainset\n",
    "heads = trainset[\"Header\"]\n",
    "heads.iloc[0].split(\"|\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"my_dataset.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    for document in all_bodies:\n",
    "        file.write(document + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lav en csv-fil til træning af headline generation model fra: https://github.com/NainiShah/News-Headline-Generation\n",
    "with open (\"articles_small.csv\", \"w\", encoding=\"utf-8\") as file:\n",
    "    for i, header in enumerate(trainset[\"Header\"]):\n",
    "        header = header.split(\"|\")[0].rstrip()\n",
    "        file.write(header + \",\" + trainset[\"Body\"].iloc[i] + \"\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sn.dk             175\n",
       "bt.dk             157\n",
       "politiken.dk      116\n",
       "version2.dk        96\n",
       "kommunen.dk        85\n",
       "finans.dk          65\n",
       "dr.dk              50\n",
       "tv2.dk             41\n",
       "information.dk     37\n",
       "Name: Domain, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.Domain.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test strings\n",
    "en_string = \"This list has overlapping features with content features. For example, word n-grams will capture the content of the text along with stylometric tendencies. Content features consist of word frequencies, word and character n-grams, hapax legomena etc. This overlap is not of concern, however, as Sari et al. show, using content features is beneficial when performing authorship attribution of news articles because journalists often have certain topics they prefer writing about. They argue that using only stylometric features is beneficial when attributing authors to texts of the same topic or genre, e.g. law text or movie reviews.\"\n",
    "da_test = 'Cecilia Lonning-Skovgaard har tidligere oplyst, at hun har søgt professionel hjælp til at forbedre sin ledelsesstil.En undersøgelse af det psykiske arbejdsmiljø i centralforvaltningen i Københavns kommune er \"rystende læsning\" og vidner om store svigt i toppen af ledelsen - herunder særligt beskæftigelses- og Integrationsborgmester Cecilia Lonning-Skovgaard (V). Sådan lyder det i et åbent brev fra en række fagforbund. Undersøgelsen dokumenterer en omfattende og fuldkommen uacceptabel, krænkende adfærd fra borgmesterens side og fjerner den sidste rest af tvivl om, hvor alvorlig og hvor uholdbar situationen er. Af undersøgelsen foretaget blandt 188 medarbejdere fremgår det, at 27 procent har oplevet krænkende adfærd, og 30 procent har været vidne til krænkende adfærd i centralforvaltningen. 47 procent af dem, der har været udsat for krænkelser og 68 procent af dem, der har været vidne til krænkende adfærd, svarer, at det er ”borgmesteren eller den øvrige politiske ledelse”, der står bag den krænkende adfærd.'\n",
    "\n",
    "da_string = ['Cecilia Lonning-Skovgaard har tidligere oplyst, at hun har søgt professionel hjælp til at forbedre sin ledelsesstil.En undersøgelse af det psykiske arbejdsmiljø i centralforvaltningen i Københavns kommune er \"rystende læsning\" og vidner om store svigt i toppen af ledelsen - herunder særligt beskæftigelses- og Integrationsborgmester Cecilia Lonning-Skovgaard (V). Sådan lyder det i et åbent brev fra en række fagforbund. Undersøgelsen dokumenterer en omfattende og fuldkommen uacceptabel, krænkende adfærd fra borgmesterens side og fjerner den sidste rest af tvivl om, hvor alvorlig og hvor uholdbar situationen er. Af undersøgelsen foretaget blandt 188 medarbejdere fremgår det, at 27 procent har oplevet krænkende adfærd, og 30 procent har været vidne til krænkende adfærd i centralforvaltningen. 47 procent af dem, der har været udsat for krænkelser og 68 procent af dem, der har været vidne til krænkende adfærd, svarer, at det er ”borgmesteren eller den øvrige politiske ledelse”, der står bag den krænkende adfærd.']\n",
    "\n",
    "train_corpus =['Til TV 2 oplyser Camilla Gregersen, formand for den akademiske fagforening DM, at Venstre bør overveje, om Cecilia Lonning-Skovgaard er den rette til posten.',\n",
    "              '- Der er et kæmpe problem i forvaltningen, og det er centreret omkring den øverste ledelse. Det sender dårlig energi i hele systemet, og det er man nødt til at handle på nu.',\n",
    "              'TV 2 har forsøgt at få en kommentar fra Venstres formand Jakob Ellemann-Jensen for at høre, hvordan han forholder sig til undersøgelsen og Cecilia Lonning-Skovgaards fremtid.']\n",
    "\n",
    "test_corpus = ['I brevet fra fagforbundene lyder opfordringen desuden, at Beskæftigelses- og Integrationsudvalget i Københavns Kommune \"øjeblikkeligt\" skal få styr på det dårlige arbejdsmiljø under borgmesteren.',\n",
    "               'TV 2 forsøger at få en kommentar fra Cecilia Lonning-Skovgaard.', \n",
    "               'Til Politiken siger hun:- Det tager jeg et kæmpe ansvar for, og det har jeg også sagt til medarbejderne her til morgen og undskyldt for. Jeg er i fuld gang med at arbejde på tonen over for medarbejderne, og jeg er også stoppet med at tage direkte kontakt til medarbejderne.',\n",
    "               'Borgmesteren har tidligere oplyst, at hun har søgt professionel hjælp til at forbedre sin ledelsesstil.',\n",
    "               'Undersøgelsen af det psykiske arbejdsmiljø blev igangsat i januar, efter at HK og DJØF i et åbent brev kritiserede Cecilia Lonning-Skovgaard for manglende indsigt i sin forvaltning og manglende forståelse for sin rolle som borgmester.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    return re.sub(r',|\\.|:|!|\\?|;', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#POS tagger trained on Danish news and media corpus\n",
    "POS_tagger_DK = spacy.load(\"da_core_news_md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NGram features - word, characters, POS-tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trains a TF-IDF vectorizer of word n-grams\n",
    "def word_ngram_vectorizer(train_corpus, n): \n",
    "    \n",
    "    vectorizer = TfidfVectorizer(max_features=3000, analyzer=\"word\", ngram_range=(n,n))\n",
    "    print(\"training vectorizer...\",rand_emot())\n",
    "    \n",
    "    X = vectorizer.fit_transform(train_corpus)\n",
    "    print(\"vectorizer fit!\", rand_emot())\n",
    "    \n",
    "    ngrams = vectorizer.get_feature_names()\n",
    "      \n",
    "    return X, vectorizer\n",
    "\n",
    "#Trains a TF-IDF vectorizer of character n-grams\n",
    "def char_ngram_vectorizer(train_corpus, n): \n",
    "    \n",
    "    vectorizer = TfidfVectorizer(max_features=3000, analyzer=\"char\", ngram_range=(n,n))\n",
    "    print(\"training vectorizer...\",rand_emot())\n",
    "    \n",
    "    X = vectorizer.fit_transform(train_corpus)\n",
    "    print(\"vectorizer fit!\", rand_emot())\n",
    "    \n",
    "    ngrams = vectorizer.get_feature_names()\n",
    "    \n",
    "    return X, vectorizer\n",
    "\n",
    "#Trains a TF-IDF vectorizer of POS n-grams. A POS corpus is generated in the function using a tagger for Danish\n",
    "def POS_ngram_vectorizer(train_corpus, n): \n",
    "    \n",
    "    #Create POS corpus\n",
    "    POS_corpus = []\n",
    "\n",
    "    for doc in train_corpus:\n",
    "        tagged_doc = POS_tagger_DK(doc) #tag each document in corpus with POS tags using spacy\n",
    "        POS_list = []\n",
    "\n",
    "        for token in tagged_doc:\n",
    "            POS_list.append(token.pos_)\n",
    "\n",
    "        #concatenate as POS tags for the document\n",
    "        POS_text = \" \".join(POS_list)\n",
    "        POS_corpus.append(POS_text)\n",
    "\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(max_features=3000, analyzer=\"word\", ngram_range=(n,n))\n",
    "    print(\"training vectorizer...\",rand_emot())\n",
    "    \n",
    "    X = vectorizer.fit_transform(POS_corpus)\n",
    "    print(\"vectorizer fit!\", rand_emot())\n",
    "    \n",
    "    ngrams = vectorizer.get_feature_names()\n",
    "    \n",
    "    # Returns \n",
    "    return X, vectorizer\n",
    "\n",
    "# Gets weights for terms based on trained vectorizer\n",
    "# Works for both word and character ngrams\n",
    "def get_tfidf_ngrams(vectorizer, test_corpus):\n",
    "    '''Returns the TF-IDF weighted ngram frequencies of test documents'''\n",
    "    #Multiple texts required\n",
    "    return vectorizer.transform(test_corpus)\n",
    "\n",
    "# Function generates POS test corpus first and then gets weights for terms based on trained vectorizer. \n",
    "def get_tfidf_POS_ngrams(vectorizer, test_corpus):\n",
    "    '''Returns the TF-IDF weighted ngram frequencies of test documents'''\n",
    "    #Create POS corpus\n",
    "    POS_corpus = []\n",
    "\n",
    "    for doc in test_corpus:\n",
    "        tagged_doc = POS_tagger_DK(doc) #tag each document in corpus with POS tags using spacy\n",
    "        POS_list = []\n",
    "\n",
    "        for token in tagged_doc:\n",
    "            POS_list.append(token.pos_)\n",
    "\n",
    "        #concatenate POS tags as one string, i.e. the documented represented as the POS tags\n",
    "        POS_text = \" \".join(POS_list)\n",
    "        POS_corpus.append(POS_text)\n",
    "    \n",
    "    #Multiple texts required\n",
    "    return vectorizer.transform(POS_corpus)\n",
    "\n",
    "def get_tfidf(vectorizer, test_corpus):\n",
    "    '''Returns the TF-IDF weighted word frequencies of test documents'''\n",
    "    #Multiple texts required\n",
    "    return vectorizer.transform(test_corpus).toarray()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training vectorizer... :P\n",
      "vectorizer fit! ¯\\_(ツ)_/¯\n",
      "training vectorizer... ( ≖.≖)\n",
      "vectorizer fit! ( ◡́.◡̀)\n",
      "training vectorizer... (T_T)\n",
      "vectorizer fit! (ㆆ_ㆆ)\n",
      "training vectorizer... ʕ•́ᴥ•̀ʔっ\n",
      "vectorizer fit! :-)\n",
      "training vectorizer... :P\n",
      "vectorizer fit! ^_^\n",
      "training vectorizer... ( ≖.≖)\n",
      "vectorizer fit! ʕ•́ᴥ•̀ʔっ\n",
      "training vectorizer... ( ͡❛ ͜ʖ ͡❛)\n",
      "vectorizer fit! x)\n",
      "training vectorizer... :-)\n",
      "vectorizer fit! OwO\n",
      "training vectorizer... OwO\n",
      "vectorizer fit! (^◡^ )\n"
     ]
    }
   ],
   "source": [
    "X_unigrams_train,unigram_vectorizer = word_ngram_vectorizer(train,1)\n",
    "X_char_unigrams_train,char_unigram_vectorizer = char_ngram_vectorizer(train,1)\n",
    "X_POS_unigram_train, POS_unigram_vectorizer = POS_ngram_vectorizer(train,1)\n",
    "\n",
    "X_bigrams_train,bigram_vectorizer = word_ngram_vectorizer(train,2)\n",
    "X_char_bigrams_train,char_bigram_vectorizer = char_ngram_vectorizer(train,2)\n",
    "X_POS_bigram_train, POS_bigram_vectorizer = POS_ngram_vectorizer(train,2)\n",
    "\n",
    "X_trigrams_train,trigram_vectorizer = word_ngram_vectorizer(train,3)\n",
    "X_char_trigrams_train,char_trigram_vectorizer = char_ngram_vectorizer(train,3)\n",
    "X_POS_trigram_train, POS_trigram_vectorizer = POS_ngram_vectorizer(train,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_unigrams_test = get_tfidf_ngrams(unigram_vectorizer, test).toarray()\n",
    "X_char_unigrams_test = get_tfidf_ngrams(char_unigram_vectorizer, test).toarray()\n",
    "X_POS_unigram_test = get_tfidf_POS_ngrams(POS_unigram_vectorizer, test).toarray()\n",
    "\n",
    "X_bigrams_test = get_tfidf_ngrams(bigram_vectorizer, test).toarray()\n",
    "X_char_bigrams_test = get_tfidf_ngrams(char_bigram_vectorizer, test).toarray()\n",
    "X_POS_bigram_test = get_tfidf_POS_ngrams(POS_bigram_vectorizer, test).toarray()\n",
    "\n",
    "X_trigrams_test = get_tfidf_ngrams(trigram_vectorizer, test).toarray()\n",
    "X_char_trigrams_test = get_tfidf_ngrams(char_trigram_vectorizer, test).toarray()\n",
    "X_POS_trigram_test = get_tfidf_POS_ngrams(POS_trigram_vectorizer, test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = hstack((X_unigrams_train,X_bigrams_train))\n",
    "X_train = hstack((X_train,X_trigrams_train))\n",
    "X_train = hstack((X_train,X_char_unigrams_train))\n",
    "X_train = hstack((X_train,X_char_bigrams_train))\n",
    "X_train = hstack((X_train,X_char_trigrams_train))\n",
    "X_train = hstack((X_train,X_POS_unigram_train))\n",
    "X_train = hstack((X_train,X_POS_bigram_train))\n",
    "X_train = hstack((X_train,X_POS_trigram_train))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.hstack((X_unigrams_test,X_bigrams_test))\n",
    "X_test = np.hstack((X_test,X_trigrams_test))\n",
    "X_test = np.hstack((X_test,X_char_unigrams_test))\n",
    "X_test = np.hstack((X_test,X_char_bigrams_test))\n",
    "X_test = np.hstack((X_test,X_char_trigrams_test))\n",
    "X_test = np.hstack((X_test,X_POS_unigram_test))\n",
    "X_test = np.hstack((X_test,X_POS_bigram_test))\n",
    "X_test = np.hstack((X_test,X_POS_trigram_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((225, 15991), (25, 15991))"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('function_words/funktionsord.txt', \"r\", encoding=\"utf-8\") as fw:\n",
    "    func_words = fw.read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, word in enumerate(func_words):\n",
    "    word = remove_punctuation(word)\n",
    "    func_words[i] = word\n",
    "    if \" \" in word:\n",
    "        func_words.remove(word)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_words(trainset, testset, function_words): \n",
    "    \n",
    "    fw_corpus = []\n",
    "    \n",
    "    for document in tqdm(trainset): \n",
    "        fw_document = \"\"\n",
    "        document = remove_punctuation(document)\n",
    "        words = document.split() #split text into words\n",
    "        \n",
    "        for word in words: \n",
    "            if word in function_words:\n",
    "                fw_document = fw_document + word + \" \"\n",
    "        \n",
    "        fw_corpus.append(fw_document)\n",
    "        \n",
    "    \n",
    "    vectorizer = TfidfVectorizer(max_features=3000, analyzer=\"word\")\n",
    "    print(\"training vectorizer...\",rand_emot())\n",
    "    \n",
    "    X = vectorizer.fit_transform(fw_corpus)\n",
    "    print(\"vectorizer fit!\", rand_emot())\n",
    "    \n",
    "    fw_vectorized = get_tfidf(vectorizer, testset)\n",
    "       \n",
    "    return X, fw_vectorized, fw_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa23a0927f624191b3cc0092ee1bad35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=225.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "training vectorizer... :-)\n",
      "vectorizer fit! x)\n"
     ]
    }
   ],
   "source": [
    "X_fw_train, X_fw_test, fw_corpus = function_words(train, test, func_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fw_corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(scipy.sparse.csr.csr_matrix, scipy.sparse.coo.coo_matrix)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_fw_train), type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((225, 138), (25, 138))"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_fw_train.shape,X_fw_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((225, 15991), (225, 138))"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.todense().shape, X_fw_train.todense().shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.hstack((X_train.todense(), X_fw_train.todense()))\n",
    "X_test = np.hstack((X_test, X_fw_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(225, 16129)\n",
      "(25, 16129)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_train.shape), print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse\n",
    "from scipy.sparse import hstack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN_classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "KNN_classifier.fit(X_train, train_labels)\n",
    "KNN_prediction = KNN_classifier.predict(X_test)\n",
    "KNN_score = KNN_classifier.score(X_test, test_labels)\n",
    "KNN_f1 = f1_score(test_labels, KNN_prediction, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_classifier = DummyClassifier(strategy=\"prior\")\n",
    "dummy_classifier.fit(X_train, train_labels)\n",
    "dummy_prediction = dummy_classifier.predict(X_test)\n",
    "dummy_score = dummy_classifier.score(X_test, test_labels)\n",
    "dummy_f1 = f1_score(test_labels, dummy_prediction, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , ..., 0.00018518, 0.        ,\n",
       "       0.        ])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RandomForest_classifier = RandomForestClassifier(random_state=42)\n",
    "RandomForest_classifier.fit(X_train, train_labels)\n",
    "RandomForest_prediction = RandomForest_classifier.predict(X_test)\n",
    "RandomForest_score = RandomForest_classifier.score(X_test, test_labels)\n",
    "RandomForest_f1 = f1_score(test_labels, RandomForest_prediction, average=\"weighted\")\n",
    "RandomForest_classifier.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "NaiveBayes_classifier = GaussianNB()\n",
    "NaiveBayes_classifier.fit(X_train, train_labels)\n",
    "NaiveBayes_prediction = NaiveBayes_classifier.predict(X_test)\n",
    "NaiveBayes_score = NaiveBayes_classifier.score(X_test, test_labels)\n",
    "NaiveBayes_f1 = f1_score(test_labels, NaiveBayes_prediction, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_classifier = svm.SVC()\n",
    "SVM_classifier.fit(X_train, train_labels)\n",
    "SVM_prediction = SVM_classifier.predict(X_test)\n",
    "SVM_score = SVM_classifier.score(X_test, test_labels)\n",
    "SVM_f1 = f1_score(test_labels, SVM_prediction, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of Newspaper attribution\n",
      "\n",
      "Accuracy scores:\n",
      " KNN: 0.36\n",
      " Dummy: 0.16\n",
      " Random Forest: 0.4\n",
      " Naive Bayes: 0.36\n",
      " SVM: 0.36 \n",
      "\n",
      "\n",
      "F1 scores:\n",
      " KNN: 0.35584859584859585\n",
      " Dummy: 0.04413793103448277\n",
      " Random Forest: 0.35171261487050964\n",
      " Naive Bayes: 0.2978431372549019\n",
      " SVM: 0.34643578643578643 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluation of Newspaper attribution\\n\")\n",
    "print(\"Accuracy scores:\\n KNN: {}\\n Dummy: {}\\n Random Forest: {}\\n Naive Bayes: {}\\n SVM: {} \\n\\n\".format(KNN_score, dummy_score, RandomForest_score, NaiveBayes_score, SVM_score))\n",
    "print(\"F1 scores:\\n KNN: {}\\n Dummy: {}\\n Random Forest: {}\\n Naive Bayes: {}\\n SVM: {} \\n\\n\".format(KNN_f1, dummy_f1, RandomForest_f1, NaiveBayes_f1, SVM_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
