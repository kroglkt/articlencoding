{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatting the data\n",
    "The data available is not perfect. Some articles have missing information. Such texts will be filtered out - The data will be loaded from a JSON and converted to a Pandas dataframe. \n",
    "\n",
    "The necessary features are: __Domain, Body, Header__ and __Byline__. URI is nice to have, but not necessary.\n",
    "\n",
    "Author: lkt259@alumni.ku.dk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, re\n",
    "from tqdm.notebook import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats.stats import pearsonr\n",
    "import random\n",
    "from difflib import SequenceMatcher\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "scrape = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Domain</th>\n",
       "      <th>Body</th>\n",
       "      <th>Header</th>\n",
       "      <th>PublicationDate</th>\n",
       "      <th>Uri</th>\n",
       "      <th>Byline</th>\n",
       "      <th>TextHash</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842325</td>\n",
       "      <td>politiken.dk</td>\n",
       "      <td>Læs artiklen senere Gemt (klik for at fjerne) Læst Giv artiklen videre Som abonnent kan du ubegrænset dele artikler med dine venner og familie. Læs mere om fordelene ved et abonnement her . FOR ABONNENTER »Der var ov...</td>\n",
       "      <td>Utilfreds passager: »Prøv selv en tur klokken 7.30 en hverdagsmorgen« - politiken.dk</td>\n",
       "      <td>2002-09-17T00:00:00</td>\n",
       "      <td>https://politiken.dk/forbrugogliv/art6088327/Utilfreds-passager-%C2%BBPr%C3%B8v-selv-en-tur-klokken-7.30-en-hverdagsmorgen%C2%AB</td>\n",
       "      <td>Annemette Grundtvig</td>\n",
       "      <td>-346867430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842324</td>\n",
       "      <td>politiken.dk</td>\n",
       "      <td>Klimamonitor Byrummonitor Skolemonitor Sundhedsmonitor Kulturmonitor Følg os Minimalist: »Ofte står jeg og mangler en ting og bliver irriteret på mig selv over, at jeg har smidt den ud« For to år siden fik forfatter ...</td>\n",
       "      <td>Minimalist: »Ofte står jeg og mangler en ting og bliver irriteret på mig selv over, at jeg har smidt den ud« - politiken.dk</td>\n",
       "      <td>2013-01-18T00:00:00</td>\n",
       "      <td>https://politiken.dk/forbrugogliv/art6287464/%C2%BBOfte-st%C3%A5r-jeg-og-mangler-en-ting-og-bliver-irriteret-p%C3%A5-mig-selv-over-at-jeg-har-smidt-den-ud%C2%AB</td>\n",
       "      <td>Annemette Grundtvig</td>\n",
       "      <td>-1346564151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>842323</td>\n",
       "      <td>politiken.dk</td>\n",
       "      <td>Læs artiklen senere Gemt (klik for at fjerne) Læst Giv artiklen videre Som abonnent kan du ubegrænset dele artikler med dine venner og familie. Læs mere om fordelene ved et abonnement her . Køb abonnement Den familie...</td>\n",
       "      <td>Digitalt forældreskab: Hvilken type er du som Facebook-forælder? - politiken.dk</td>\n",
       "      <td>2006-05-17T00:00:00</td>\n",
       "      <td>https://politiken.dk/forbrugogliv/art5934818/Hvilken-type-er-du-som-Facebook-for%C3%A6lder</td>\n",
       "      <td>Annemette Grundtvig</td>\n",
       "      <td>1473032676</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id        Domain  \\\n",
       "0  842325  politiken.dk   \n",
       "1  842324  politiken.dk   \n",
       "2  842323  politiken.dk   \n",
       "\n",
       "                                                                                                                                                                                                                          Body  \\\n",
       "0  Læs artiklen senere Gemt (klik for at fjerne) Læst Giv artiklen videre Som abonnent kan du ubegrænset dele artikler med dine venner og familie. Læs mere om fordelene ved et abonnement her . FOR ABONNENTER »Der var ov...   \n",
       "1  Klimamonitor Byrummonitor Skolemonitor Sundhedsmonitor Kulturmonitor Følg os Minimalist: »Ofte står jeg og mangler en ting og bliver irriteret på mig selv over, at jeg har smidt den ud« For to år siden fik forfatter ...   \n",
       "2  Læs artiklen senere Gemt (klik for at fjerne) Læst Giv artiklen videre Som abonnent kan du ubegrænset dele artikler med dine venner og familie. Læs mere om fordelene ved et abonnement her . Køb abonnement Den familie...   \n",
       "\n",
       "                                                                                                                        Header  \\\n",
       "0                                         Utilfreds passager: »Prøv selv en tur klokken 7.30 en hverdagsmorgen« - politiken.dk   \n",
       "1  Minimalist: »Ofte står jeg og mangler en ting og bliver irriteret på mig selv over, at jeg har smidt den ud« - politiken.dk   \n",
       "2                                              Digitalt forældreskab: Hvilken type er du som Facebook-forælder? - politiken.dk   \n",
       "\n",
       "       PublicationDate  \\\n",
       "0  2002-09-17T00:00:00   \n",
       "1  2013-01-18T00:00:00   \n",
       "2  2006-05-17T00:00:00   \n",
       "\n",
       "                                                                                                                                                                Uri  \\\n",
       "0                                  https://politiken.dk/forbrugogliv/art6088327/Utilfreds-passager-%C2%BBPr%C3%B8v-selv-en-tur-klokken-7.30-en-hverdagsmorgen%C2%AB   \n",
       "1  https://politiken.dk/forbrugogliv/art6287464/%C2%BBOfte-st%C3%A5r-jeg-og-mangler-en-ting-og-bliver-irriteret-p%C3%A5-mig-selv-over-at-jeg-har-smidt-den-ud%C2%AB   \n",
       "2                                                                        https://politiken.dk/forbrugogliv/art5934818/Hvilken-type-er-du-som-Facebook-for%C3%A6lder   \n",
       "\n",
       "                Byline    TextHash  \n",
       "0  Annemette Grundtvig  -346867430  \n",
       "1  Annemette Grundtvig -1346564151  \n",
       "2  Annemette Grundtvig  1473032676  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load data - and show 120 characters of the body.\n",
    "data = pd.read_json(r'data/data_0.json')\n",
    "pd.set_option('display.max_colwidth', 220)\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing duplicates and empty bodies\n",
    "We don't want duplicate articles. First, we'll remove entries with the same URI and body. I save the entries, which have empty bodies and unique URI's - Perhaps we can get the body online, if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With duplicates: (10000, 8)\n",
      "Without duplicates: (9070, 8)\n"
     ]
    }
   ],
   "source": [
    "def print_stats():\n",
    "    print(f\"{data.shape[0]} entries.\")\n",
    "    print(f\"{data.Byline.unique().shape[0]} authors.\")\n",
    "    print(f\"{data.Domain.unique().shape[0]} domains.\")\n",
    "\n",
    "def same_uri(uri):\n",
    "    return data[data['Uri']==uri]\n",
    "\n",
    "def remove_duplicates(data):\n",
    "    print(\"With duplicates:\", data.shape)\n",
    "    data = data.drop_duplicates(subset=[\"Body\", \"Uri\"]) #Remove entries with same body and URI\n",
    "    empty_bodies = data[data['Body'] == '']    #Save empty bodied entries in another dataframe.\n",
    "    data = data[data['Body'] != '']\n",
    "    empty_bodies = empty_bodies[~empty_bodies['Uri'].isin(data['Uri'])] #Remove empty bodies, which are present in data.\n",
    "    duplicate_bodies = data[data.duplicated(subset=[\"Body\"])]\n",
    "    data = data.drop_duplicates(subset=['Body']) #Can remove all duplicate bodies here, after None-bodies are removed.\n",
    "    data = data.drop_duplicates(subset=[\"Uri\"]) #Remove entries with same URI\n",
    "    \n",
    "    print(\"Without duplicates:\", data.shape)\n",
    "    return data, empty_bodies, duplicate_bodies\n",
    "\n",
    "data, _, _ = remove_duplicates(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing unknown authors, headers and domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_missing_feats(data):\n",
    "    no_author, no_header, no_domain = None, None, None\n",
    "    \n",
    "    if data['Byline'].isnull().values.any(): #Missing authors\n",
    "        no_author = data[data['Byline'].isnull()]\n",
    "        data = data.dropna(subset=['Byline'])\n",
    "        print(\"Without unknown authors:\", data.shape)\n",
    "\n",
    "    if data['Header'].isnull().values.any():\n",
    "        no_header = data[data['Header'].isnull()]\n",
    "        data = data.dropna(subset=['Header'])\n",
    "        print(\"Without unknown title:\", data.shape)\n",
    "    \n",
    "    data = data[data.Header != '']\n",
    "\n",
    "    if data['Domain'].isnull().values.any():\n",
    "        no_domain = data[data['Domain'].isnull()]\n",
    "        data = data.dropna(subset=['Domain'])\n",
    "        print(\"Without unknown domains:\", data.shape)\n",
    "        \n",
    "    return data, no_author, no_header, no_domain\n",
    "\n",
    "data, _, _, _ = remove_missing_feats(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying to all data files.\n",
    "After investigating the data, let's apply the changes to the files. No need in formatting multiple times. Also, let's see how the distributions are when collecting all data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11596170fe884fbc9a522db519a5a41f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "808066 entries.\n",
      "66994 authors.\n",
      "268 domains.\n"
     ]
    }
   ],
   "source": [
    "#Load all data into one big dataframe. Oh yes, it is possible!\n",
    "files = os.listdir('data')\n",
    "files = [x for x in files if x[-4:]=='json'] #only take .json files.\n",
    "\n",
    "data = pd.read_json('data/'+files[0])\n",
    "for file in tqdm(files[1:]):\n",
    "    data = pd.concat([data, pd.read_json('data/'+file)])\n",
    "\n",
    "print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Used for plotting later...\n",
    "body_lengths = data['Body'].str.len()\n",
    "y2, bin_edges = np.histogram(body_lengths, bins=150, range=(0,15000))\n",
    "x2 = 0.5*(bin_edges[1:] + bin_edges[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove duplicated and missing data\n",
    "Some data did not meet the requirements. How much data do we have left?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With duplicates: (808066, 8)\n",
      "Without duplicates: (431531, 8)\n",
      "Without unknown authors: (398856, 8)\n",
      "With bad authors removed: (348751, 8)\n",
      "348751 entries.\n",
      "7849 authors.\n",
      "205 domains.\n"
     ]
    }
   ],
   "source": [
    "data, empty_bodies, duplicate_bodies = remove_duplicates(data)\n",
    "data, no_author, no_header, no_domain = remove_missing_feats(data)\n",
    "\n",
    "#Removing bad authors. Baaad author, shame on you!\n",
    "bad_byline = data[data['Byline']==data['Uri']]\n",
    "data = data[data['Byline']!=data['Uri']]\n",
    "print(\"With bad authors removed:\", data.shape)\n",
    "print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique authors:  7776\n",
      "Anne Sophia Hermansen                                  13559\n",
      "Karim Pedersen                                          7157\n",
      "Thomas Treo                                             6172\n",
      "Mads Elkær                                              4898\n",
      "Benny Baagø                                             4271\n",
      "                                                       ...  \n",
      "Emil Bergløv | Sofie Synnøve Herschend                     1\n",
      "Jo Carlsenn | Rie Carlsen                                  1\n",
      "Kasper Schütt-Jensen | Mads Frost                          1\n",
      "Steen Nedell Christensen | Jesper Haue Hansen              1\n",
      "Michael Hjøllund | Jacob Haislund | Jesper Kongstad        1\n",
      "Name: Byline, Length: 7771, dtype: int64 \n",
      "...\n"
     ]
    }
   ],
   "source": [
    "#Titel-ize Authors!\n",
    "data.Byline = data.Byline.str.title()\n",
    "print(\"Unique authors: \", len(data['Byline'].value_counts()))\n",
    "print(data['Byline'].value_counts()[:-5],\"\\n...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "448 empty bodies\n",
      "32675 without author\n",
      "28789 malformed authors\n"
     ]
    }
   ],
   "source": [
    "print(len(empty_bodies), \"empty bodies\")\n",
    "print(len(no_author), \"without author\")\n",
    "#print(len(no_header), \"without header\")\n",
    "#print(len(no_domain), \"without domain\")\n",
    "print(len(bad_byline), \"malformed authors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Author cleanup! Removing multiple authors.\n",
    "Some entries have multiple auhthors. How big a problem does this cause?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries with multiple authors (and titles, cities): 11275\n",
      "Unique combinations of authors: 4747\n"
     ]
    }
   ],
   "source": [
    "authors = list(data['Byline'])[1:] #First entry is empty.\n",
    "multiple_authors_candidates = []\n",
    "for author in authors:\n",
    "    if any(x in author for x in ['|',',']):\n",
    "        multiple_authors_candidates.append(author)\n",
    "\n",
    "print(\"Entries with multiple authors (and titles, cities):\" ,len(multiple_authors_candidates))\n",
    "print(f\"Unique combinations of authors: {len(set(multiple_authors_candidates))}\")\n",
    "\n",
    "#multiple_authors = [re.split('\\||,',x) for x in multiple_authors]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now! Let us remove entries with multiple authors.**\n",
    "\n",
    "Some authors are actually not multiple authors, but cities and titles. These entries will be kept, but stripped from everything except the name. We don't want multiple authors, as this project focuses on single authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2896 authors removed by '|-rule'\n",
      "1463 author combinations removed by filter.\n",
      "Kept 215 authors (Not sure if they are new authors yet).\n",
      "139 authors already present in data by themselves.\n"
     ]
    }
   ],
   "source": [
    "#Read txt file with words that should not be masked.\n",
    "with open('data/additional/author_filter.txt', encoding='utf-8') as f:\n",
    "    lst = f.read()\n",
    "    lst = lst.split('\\n')\n",
    "\n",
    "author_filter = lst\n",
    "\n",
    "print(f\"{len(set([x for x in multiple_authors_candidates if '|' in x]))} authors removed by '|-rule'\")\n",
    "bar_rule = [x for x in multiple_authors_candidates if '|' in x]\n",
    "multiple_authors_candidates = [x for x in multiple_authors_candidates if '|' not in x] #Remove entries with | in them.\n",
    "\n",
    "new_authors_candidates = []\n",
    "author_names_without_city = [] #Save the new, stripped name and the old one, to edit data fterwards.\n",
    "\n",
    "for candidate in multiple_authors_candidates:\n",
    "    names = candidate.split(',') #Split by , to separate authors or cities    \n",
    "    \n",
    "    append = True\n",
    "    \n",
    "    #Loop through names\n",
    "    for name in names:\n",
    "        name = name.strip()\n",
    "        if len(name.split())>1: #Name is not two or more names.\n",
    "            if any(ele in name for ele in author_filter): #Do not append if name found in filter.\n",
    "                append = False\n",
    "        else:\n",
    "            append = False\n",
    "    \n",
    "    if append:\n",
    "        new_authors_candidates.append(candidate)\n",
    "    else:\n",
    "        #Append to names list without cities. \n",
    "        if any(ele in names[0] for ele in author_filter): #If first name appears in filter, use second name.\n",
    "            author_names_without_city.append((names[1], candidate))\n",
    "        else:\n",
    "            author_names_without_city.append((names[0], candidate))\n",
    "            \n",
    "print(f\"{len(set(new_authors_candidates))} author combinations removed by filter.\")\n",
    "print(f\"Kept {len(set([x[0] for x in author_names_without_city]))} authors (Not sure if they are new authors yet).\")\n",
    "\n",
    "already_there_hon = 0\n",
    "uniques = data['Byline'].unique()\n",
    "for i in set([x[0] for x in author_names_without_city]):\n",
    "    if i in uniques:\n",
    "        already_there_hon += 1\n",
    "        \n",
    "print(f\"{already_there_hon} authors already present in data by themselves.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting names...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df95991a333d4350baae80bd6311a9a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1442 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Drop multiple authors from data.\n",
    "data = data[data['Byline'].isin(new_authors_candidates) == False]\n",
    "data = data[data['Byline'].isin(bar_rule) == False]\n",
    "\n",
    "#Convert wrong names to right names in data.\n",
    "#Badly optimised...\n",
    "print(\"Converting names...\")\n",
    "for entry in tqdm(author_names_without_city):\n",
    "    name = entry[0]\n",
    "    wrong_name = entry[1]\n",
    "    \n",
    "    data.loc[data[\"Byline\"] == wrong_name, \"Byline\"] = name\n",
    "print(\"Names are converted! That was slow.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove authors: Redaktionen and Ritzau /Nyheder\n",
    "data = data[data.Byline.isin(['Redaktionen', 'Ritzau /Nyheder']) == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Unique Entries\n",
    "We don't want unique author or domain names. We need **at least** two entries for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop rows where domain only occurs once.\n",
    "d = data['Domain'].value_counts()\n",
    "data = data[data['Domain'].isin(d[d<2].keys()) == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find which author names that can be salvaged. (Are they calpitalized?)\n",
    "d = data['Byline'].value_counts()\n",
    "unique_authors = d[d<2].keys()\n",
    "common_authors = d[d>2].keys()\n",
    "\n",
    "for author in unique_authors:\n",
    "    author = author.strip()\n",
    "    if author.istitle():\n",
    "        continue\n",
    "    if author.isupper() or author.islower():\n",
    "        if author.title() in common_authors:\n",
    "            data.loc[data[\"Byline\"] == author, \"Byline\"] = author.title()\n",
    "\n",
    "#Remove any entries with single-occuring authors.\n",
    "data = data[data['Byline'].isin(unique_authors) == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop rows where domain only occurs once.\n",
    "d = data['Domain'].value_counts()\n",
    "data = data[data['Domain'].isin(d[d<2].keys()) == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional cleanup\n",
    "Removing duplicated headers and foreign domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = data['Header'].value_counts()\n",
    "data = data[data['Header'].isin(h[h>20][1:].keys())==False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove foreign domains** - We don't want German texts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domains = ('.dk', 'nordicwomeninfilm.com', 'linkedin.com', 'betxpert.com', 'kommunikation.com', 'radar.com', 'ea.com', 'ue.com','ce.com', '.international', 'e.nu')\n",
    "print(data[data['Domain'].str.endswith(domains)==False].shape[0], \"entries with foreign domains.\")\n",
    "data = data[data['Domain'].str.endswith(domains)]\n",
    "print_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Duplicate domains:__ computerworld.dk has 3 domains of different formats. Unify them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data.Domain.isin([\"Computerworld.dk\", \"www.computerworld.dk\"]), 'Domain'] = 'computerworld.dk'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[(data.Domain=='bt.dk') & (data.Header=='Din profil er oprettet')].Uri.head(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape 31k articles\n",
    "Some articles have the same header, same body. The domains are limited to bt.dk and berlingske.dk - making a scraper is not too hard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove = []\n",
    "import traceback\n",
    "\n",
    "if scrape:\n",
    "    try:\n",
    "        data = data.reset_index()\n",
    "    except:\n",
    "        pass\n",
    "    import SeleniumScraper\n",
    "    \n",
    "    treo = data[data['Byline']=='Thomas Treo']\n",
    "    anne = data[data['Byline']=='Anne Sophia Hermansen']\n",
    "    corrupt_data = data[data['Header']=='Din profil er oprettet']\n",
    "    corrupt_data = pd.concat((corrupt_data, treo, anne))\n",
    "    \n",
    "    errors = []\n",
    "    _404s = []\n",
    "    \n",
    "    i = 0\n",
    "    o = corrupt_data.shape[0]\n",
    "\n",
    "    for idx, article in tqdm(corrupt_data.iterrows()):\n",
    "        url = article.Uri\n",
    "\n",
    "        try:\n",
    "            header, body, author = SeleniumScraper.get_content(url)\n",
    "\n",
    "        except Exception as e:\n",
    "            i += 1\n",
    "            errors.append((url, e))\n",
    "            remove.append(article.Id)\n",
    "            print(traceback.format_exc())\n",
    "            continue\n",
    "    \n",
    "        if type(author) == list:\n",
    "            errors.append((url, f\"MULTIPLE AUTHORS:{author}\"))\n",
    "            remove.append(article.Id)\n",
    "            continue\n",
    "            \n",
    "        if author != '404':\n",
    "            data.iloc[idx, data.columns.get_loc('Header')] = header\n",
    "        else:\n",
    "            _404s.append(url)\n",
    "            continue\n",
    "            \n",
    "        if body != '404':\n",
    "            data.iloc[idx, data.columns.get_loc('Body')] = body\n",
    "            \n",
    "        if author != '404':\n",
    "            data.iloc[idx, data.columns.get_loc('Byline')] = author\n",
    "\n",
    "        print(header, f\"({i}/{o})\")\n",
    "        i += 1\n",
    "        \n",
    "data = data[data.Id.isin(remove)==False]\n",
    "data.to_json(r'data/additional/scraped_data_raw.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define junk text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function will identify phrases that are repeated throughout the domains. \n",
    "#However, it will have multiple duplicate matches, making the conversion slow... Oh well...\n",
    "def get_junk_by_domain(domain, verbose=False):\n",
    "    bodies = data[data.Domain==domain].Body.to_list()\n",
    "    junk = {}\n",
    "    if verbose:\n",
    "        print(\"Started on\", domain)\n",
    "    for i in range(len(bodies)//4):\n",
    "        randomindex = None\n",
    "\n",
    "        while True:\n",
    "            randomindex = random.randint(0,len(bodies)-1)\n",
    "            if randomindex != i:\n",
    "                break\n",
    "\n",
    "        string1 = bodies[i]\n",
    "        string2 = bodies[randomindex]\n",
    "\n",
    "        #match = SequenceMatcher(None, string1, string2).find_longest_match(0, len(string1), 0, len(string2))\n",
    "\n",
    "        matches = SequenceMatcher(None, string1, string2).get_matching_blocks()\n",
    "                \n",
    "        for match in matches[:-1]:\n",
    "            if match.size > 20:\n",
    "                matchstr = string1[match.a: match.a + match.size] \n",
    "                if matchstr not in junk:\n",
    "                    junk[matchstr] = 1\n",
    "                else:\n",
    "                    junk[matchstr] += 1\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Found {len(junk)} sentences to remove in {domain}...\")\n",
    "    \n",
    "    keys = list(junk.keys())\n",
    "    if verbose:\n",
    "        print(keys)\n",
    "    junk_list = []\n",
    "                \n",
    "    for key in keys:\n",
    "        if junk[key] > 2:\n",
    "            junk_list.append(key)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Narrowed that down to {len(junk_list)} junk sentences.\")\n",
    "    \n",
    "    return junk_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_junk = {}\n",
    "for domain in tqdm(data.Domain.unique()):\n",
    "    domain_junk[domain] = get_junk_by_domain(domain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove author and newspaper names\n",
    "We don't want author and newspaper names in bodies and headlines. Let's DEMOLISH them. Also remove URL's and HTML tags and junk text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = data.reset_index()\n",
    "placeholder_author = '{FORFATTER}'\n",
    "placeholder_domain = '{AVIS}'\n",
    "\n",
    "num_new_headers = 0\n",
    "num_new_bodies = 0\n",
    "junk_removed = 0\n",
    "\n",
    "#Remove URL and HTML junk from body\n",
    "def strip_tags(text):\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    return text\n",
    "\n",
    "def strip_junk(text, junk):\n",
    "    reg = re.compile('|'.join(map(re.escape, junk)), re.IGNORECASE)\n",
    "    return reg.sub(\"\", text)\n",
    "\n",
    "for idx, article in tqdm(data.iterrows()):\n",
    "    domain_no_dot = article.Domain.split('.')[0]\n",
    "\n",
    "    new_header = article.Header\n",
    "    new_body = article.Body\n",
    "\n",
    "    reg_author = re.compile(re.escape(article.Byline), re.IGNORECASE)\n",
    "    reg_domain = re.compile(re.escape(domain_no_dot), re.IGNORECASE) \n",
    "\n",
    "    #Change body\n",
    "    if reg_author.search(article.Body) or reg_domain.search(article.Body):\n",
    "        num_new_bodies += 1\n",
    "\n",
    "        new_body = reg_author.sub(placeholder_author, article.Body)\n",
    "        new_body = reg_domain.sub(placeholder_domain, article.Body)\n",
    "    \n",
    "    if domain_junk[article.Domain]:\n",
    "        new_body = strip_junk(new_body, domain_junk[article.Domain])\n",
    "        junk_removed += 1\n",
    "    \n",
    "    data.iloc[idx, data.columns.get_loc('Body')] = strip_tags(new_body)\n",
    "    \n",
    "    #Change header\n",
    "    if reg_author.search(article.Header) or reg_domain.search(article.Header):\n",
    "        num_new_headers += 1\n",
    "\n",
    "        new_header = reg_author.sub(placeholder_author, article.Header)\n",
    "        new_header = reg_domain.sub(placeholder_domain, article.Header)\n",
    "\n",
    "    data.iloc[idx, data.columns.get_loc('Header')] = new_header\n",
    "\n",
    "print(f\"Altered {num_new_headers} headers.\")\n",
    "print(f\"And {num_new_bodies} bodies.\")\n",
    "print(f\"And {junk_removed} pieces of junk removed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the formatted data as a new JSON\n",
    "Very important, we do not want to wait for the scraper a second time..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10000\n",
    "for g, df in data.groupby(np.arange(len(data)) // n):\n",
    "    df.to_json(r'data/additional/scraped/data_%s.json'%g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insights in data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Body lengths\n",
    "Let's see the distribution of the body lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body_lengths = data['Body'].str.len()\n",
    "\n",
    "desc = \"A dent in article counts at 250 characters\\nNot true\"\n",
    "\n",
    "y, bin_edges = np.histogram(body_lengths, bins=150, range=(0,15000))\n",
    "x = 0.5*(bin_edges[1:] + bin_edges[:-1])\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(14,8))\n",
    "plt.bar(x,y,width=np.diff(bin_edges), label=\"Preprocessed Data\")\n",
    "plt.step(x2+30, y2*0.41, 'k', alpha=0.8, label=\"Before Preprocessing\")\n",
    "plt.title(\"Histogram of Article Lengths\", fontsize=18)\n",
    "plt.xlabel(\"Character count\", fontsize=16)\n",
    "plt.ylabel(\"Article count\", fontsize=16)\n",
    "plt.legend(fontsize=14);\n",
    "#plt.text(0.7, 0.7, desc, horizontalalignment='center',verticalalignment='center', transform=ax.transAxes, fontsize=16)\n",
    "#plt.plot([250,10000], [30,240], '--', c='red', alpha=.6)\n",
    "#Ændr, så det passer med den endelige data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Author contributions\n",
    "How many articles have the authors written?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = list(data['Byline'].value_counts())\n",
    "\n",
    "# plt.yscale('symlog')\n",
    "# plt.xlim(-100,4000)\n",
    "\n",
    "y, bin_edges = np.histogram(counts, bins=200)\n",
    "x = 0.5*(bin_edges[1:] + bin_edges[:-1])\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(14,8))\n",
    "plt.bar(x,y,width=np.diff(bin_edges))\n",
    "\n",
    "plt.yscale('symlog')\n",
    "plt.title(\"Histogram Over Number of Articles per Author\", fontsize=18)\n",
    "plt.xlabel(\"Number of articles\", fontsize=16)\n",
    "plt.ylabel(\"Number of authors\", fontsize=16)\n",
    "\n",
    " desc = \"Outlier: Anne Sophia Hermansen\\n13535 articles written.\"\n",
    "# plt.text(0.7, 0.7, desc, horizontalalignment='center',verticalalignment='center', transform=ax.transAxes, fontsize=16)\n",
    "# plt.plot([40000,53000], [120,1], '--', c='red', alpha=.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Author body length\n",
    "Do authors have certain body lengths? In articles, not their actual bodies. Let us make a plot to see if there is a correlation between article count and body length. But first, we need to find the average body length per author. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['body_length', 'author'])\n",
    "df['body_length'] = body_lengths\n",
    "df['author'] = data['Byline']\n",
    "df['author'].replace('', np.nan, inplace=True)\n",
    "df = df.dropna()\n",
    "avg_body_lengths = list(df.groupby(['author']).mean()['body_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(avg_body_lengths, bins=200);\n",
    "plt.title(\"Histogram Over Average Article Length per Author\", fontsize=16)\n",
    "plt.xlabel(\"Average length\", fontsize=14)\n",
    "plt.ylabel(\"Number of authors\", fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(index=data['Byline'].unique(),columns=['avg_len', 'article_count'])\n",
    "avg_body_length_series = df.groupby(['author']).mean()['body_length']\n",
    "article_count_series = data['Byline'].value_counts()\n",
    "\n",
    "for i in data['Byline'].unique():\n",
    "    if i == '':\n",
    "        continue\n",
    "    \n",
    "    avg_len = avg_body_length_series[i]\n",
    "    article_count = article_count_series[i]\n",
    "    df2.loc[i] = [avg_len, article_count]\n",
    "\n",
    "df2.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Correlation between average length and article count**\n",
    "\n",
    "Make a dataframe with author, average article length, article count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df2.avg_len\n",
    "y = df2.article_count\n",
    "plt.scatter(x[y<2000], y[y<2000], marker='.')\n",
    "plt.ylabel(\"Article count\")\n",
    "plt.xlabel(\"Article length\")\n",
    "\n",
    "x = x.to_numpy(dtype=float)\n",
    "y = y.to_numpy(dtype=float)\n",
    "x = np.nan_to_num(x)\n",
    "y = np.nan_to_num(y)\n",
    "\n",
    "corr, sig = pearsonr(x,y)\n",
    "print(\"Pearsin correlation:\", corr)\n",
    "\n",
    "plt.title(\"No Linear Correlation Between Article Length and Article Count\", fontsize=15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Byline.str.len().median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treo = data[data['Byline']=='Thomas Treo']\n",
    "anne = data[data['Byline']=='Anne Sophia Hermansen']\n",
    "corrupt_data = data[data['Header']=='Din profil er oprettet']\n",
    "corrupt_data = pd.concat((corrupt_data, treo, anne))\n",
    "corrupt_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = list(data.PublicationDate.str[:4])\n",
    "int_dates = []\n",
    "old_af = []\n",
    "future = []\n",
    "for i in dates:\n",
    "    try:\n",
    "        inti = int(i)\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    if inti < 1995:\n",
    "        old_af.append(inti)\n",
    "        continue\n",
    "    if inti > 2022:\n",
    "        future.append(inti)\n",
    "        continue\n",
    "    int_dates.append(inti)\n",
    "\n",
    "print(len(old_af), \"articles from before 1995\")\n",
    "print(len(future), \"articles from the future\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(14,8))\n",
    "sns.distplot(int_dates, hist=False)\n",
    "plt.title(\"Distribtion of Publication Dates\", fontsize=18)\n",
    "plt.xlabel(\"Publication Year\", fontsize=16)\n",
    "plt.ylabel(\"Density\", fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json('data/additional/preprocessed_data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.median(future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
