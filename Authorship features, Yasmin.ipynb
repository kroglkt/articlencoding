{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test string\n",
    "string = \"This list has overlapping features with content features. For example, word n-grams will capture the content of the text along with stylometric tendencies. Content features consist of word frequencies, word and character n-grams, hapax legomena etc. This overlap is not of concern, however, as Sari et al. \\cite{Sari2018} show, using content features is beneficial when performing authorship attribution of news articles because journalists often have certain topics they prefer writing about. They argue that using only stylometric features is beneficial when attributing authors to texts of the same topic or genre, e.g. law text or movie reviews.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'list',\n",
       " 'has',\n",
       " 'overlapping',\n",
       " 'features',\n",
       " 'with',\n",
       " 'content',\n",
       " 'features',\n",
       " 'For',\n",
       " 'example',\n",
       " 'word',\n",
       " 'n',\n",
       " 'grams',\n",
       " 'will',\n",
       " 'capture',\n",
       " 'the',\n",
       " 'content',\n",
       " 'of',\n",
       " 'the',\n",
       " 'text',\n",
       " 'along',\n",
       " 'with',\n",
       " 'stylometric',\n",
       " 'tendencies',\n",
       " 'Content',\n",
       " 'features',\n",
       " 'consist',\n",
       " 'of',\n",
       " 'word',\n",
       " 'frequencies',\n",
       " 'word',\n",
       " 'and',\n",
       " 'character',\n",
       " 'n',\n",
       " 'grams',\n",
       " 'hapax',\n",
       " 'legomena',\n",
       " 'etc',\n",
       " 'This',\n",
       " 'overlap',\n",
       " 'is',\n",
       " 'not',\n",
       " 'of',\n",
       " 'concern',\n",
       " 'however',\n",
       " 'as',\n",
       " 'Sari',\n",
       " 'et',\n",
       " 'al',\n",
       " 'cite',\n",
       " 'Sari2018',\n",
       " 'show',\n",
       " 'using',\n",
       " 'content',\n",
       " 'features',\n",
       " 'is',\n",
       " 'beneficial',\n",
       " 'when',\n",
       " 'performing',\n",
       " 'authorship',\n",
       " 'attribution',\n",
       " 'of',\n",
       " 'news',\n",
       " 'articles',\n",
       " 'because',\n",
       " 'journalists',\n",
       " 'often',\n",
       " 'have',\n",
       " 'certain',\n",
       " 'topics',\n",
       " 'they',\n",
       " 'prefer',\n",
       " 'writing',\n",
       " 'about',\n",
       " 'They',\n",
       " 'argue',\n",
       " 'that',\n",
       " 'using',\n",
       " 'only',\n",
       " 'stylometric',\n",
       " 'features',\n",
       " 'is',\n",
       " 'beneficial',\n",
       " 'when',\n",
       " 'attributing',\n",
       " 'authors',\n",
       " 'to',\n",
       " 'texts',\n",
       " 'of',\n",
       " 'the',\n",
       " 'same',\n",
       " 'topic',\n",
       " 'or',\n",
       " 'genre',\n",
       " 'e',\n",
       " 'g',\n",
       " 'law',\n",
       " 'text',\n",
       " 'or',\n",
       " 'movie',\n",
       " 'reviews']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "tokenizer.tokenize(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams = ngrams(string.split(), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('This',)\n",
      "('list',)\n",
      "('has',)\n",
      "('overlapping',)\n",
      "('features',)\n",
      "('with',)\n",
      "('content',)\n",
      "('features.',)\n",
      "('For',)\n",
      "('example,',)\n",
      "('word',)\n",
      "('n-grams',)\n",
      "('will',)\n",
      "('capture',)\n",
      "('the',)\n",
      "('content',)\n",
      "('of',)\n",
      "('the',)\n",
      "('text',)\n",
      "('along',)\n",
      "('with',)\n",
      "('stylometric',)\n",
      "('tendencies.',)\n",
      "('Content',)\n",
      "('features',)\n",
      "('consist',)\n",
      "('of',)\n",
      "('word',)\n",
      "('frequencies,',)\n",
      "('word',)\n",
      "('and',)\n",
      "('character',)\n",
      "('n-grams,',)\n",
      "('hapax',)\n",
      "('legomena',)\n",
      "('etc.',)\n",
      "('This',)\n",
      "('overlap',)\n",
      "('is',)\n",
      "('not',)\n",
      "('of',)\n",
      "('concern,',)\n",
      "('however,',)\n",
      "('as',)\n",
      "('Sari',)\n",
      "('et',)\n",
      "('al.',)\n",
      "('\\\\cite{Sari2018}',)\n",
      "('show,',)\n",
      "('using',)\n",
      "('content',)\n",
      "('features',)\n",
      "('is',)\n",
      "('beneficial',)\n",
      "('when',)\n",
      "('performing',)\n",
      "('authorship',)\n",
      "('attribution',)\n",
      "('of',)\n",
      "('news',)\n",
      "('articles',)\n",
      "('because',)\n",
      "('journalists',)\n",
      "('often',)\n",
      "('have',)\n",
      "('certain',)\n",
      "('topics',)\n",
      "('they',)\n",
      "('prefer',)\n",
      "('writing',)\n",
      "('about.',)\n",
      "('They',)\n",
      "('argue',)\n",
      "('that',)\n",
      "('using',)\n",
      "('only',)\n",
      "('stylometric',)\n",
      "('features',)\n",
      "('is',)\n",
      "('beneficial',)\n",
      "('when',)\n",
      "('attributing',)\n",
      "('authors',)\n",
      "('to',)\n",
      "('texts',)\n",
      "('of',)\n",
      "('the',)\n",
      "('same',)\n",
      "('topic',)\n",
      "('or',)\n",
      "('genre,',)\n",
      "('e.g.',)\n",
      "('law',)\n",
      "('text',)\n",
      "('or',)\n",
      "('movie',)\n",
      "('reviews.',)\n"
     ]
    }
   ],
   "source": [
    "for word in unigrams:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
