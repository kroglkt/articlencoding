{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import spacy \n",
    "from spacy import displacy\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test strings\n",
    "en_string = \"This list has overlapping features with content features. For example, word n-grams will capture the content of the text along with stylometric tendencies. Content features consist of word frequencies, word and character n-grams, hapax legomena etc. This overlap is not of concern, however, as Sari et al. show, using content features is beneficial when performing authorship attribution of news articles because journalists often have certain topics they prefer writing about. They argue that using only stylometric features is beneficial when attributing authors to texts of the same topic or genre, e.g. law text or movie reviews.\"\n",
    "da_test = 'Cecilia Lonning-Skovgaard har tidligere oplyst, at hun har søgt professionel hjælp til at forbedre sin ledelsesstil.En undersøgelse af det psykiske arbejdsmiljø i centralforvaltningen i Københavns kommune er \"rystende læsning\" og vidner om store svigt i toppen af ledelsen - herunder særligt beskæftigelses- og Integrationsborgmester Cecilia Lonning-Skovgaard (V). Sådan lyder det i et åbent brev fra en række fagforbund. Undersøgelsen dokumenterer en omfattende og fuldkommen uacceptabel, krænkende adfærd fra borgmesterens side og fjerner den sidste rest af tvivl om, hvor alvorlig og hvor uholdbar situationen er. Af undersøgelsen foretaget blandt 188 medarbejdere fremgår det, at 27 procent har oplevet krænkende adfærd, og 30 procent har været vidne til krænkende adfærd i centralforvaltningen. 47 procent af dem, der har været udsat for krænkelser og 68 procent af dem, der har været vidne til krænkende adfærd, svarer, at det er ”borgmesteren eller den øvrige politiske ledelse”, der står bag den krænkende adfærd.'\n",
    "\n",
    "da_string = ['Cecilia Lonning-Skovgaard har tidligere oplyst, at hun har søgt professionel hjælp til at forbedre sin ledelsesstil.En undersøgelse af det psykiske arbejdsmiljø i centralforvaltningen i Københavns kommune er \"rystende læsning\" og vidner om store svigt i toppen af ledelsen - herunder særligt beskæftigelses- og Integrationsborgmester Cecilia Lonning-Skovgaard (V). Sådan lyder det i et åbent brev fra en række fagforbund. Undersøgelsen dokumenterer en omfattende og fuldkommen uacceptabel, krænkende adfærd fra borgmesterens side og fjerner den sidste rest af tvivl om, hvor alvorlig og hvor uholdbar situationen er. Af undersøgelsen foretaget blandt 188 medarbejdere fremgår det, at 27 procent har oplevet krænkende adfærd, og 30 procent har været vidne til krænkende adfærd i centralforvaltningen. 47 procent af dem, der har været udsat for krænkelser og 68 procent af dem, der har været vidne til krænkende adfærd, svarer, at det er ”borgmesteren eller den øvrige politiske ledelse”, der står bag den krænkende adfærd.']\n",
    "\n",
    "train_corpus =['Til TV 2 oplyser Camilla Gregersen, formand for den akademiske fagforening DM, at Venstre bør overveje, om Cecilia Lonning-Skovgaard er den rette til posten.',\n",
    "              '- Der er et kæmpe problem i forvaltningen, og det er centreret omkring den øverste ledelse. Det sender dårlig energi i hele systemet, og det er man nødt til at handle på nu.',\n",
    "              'TV 2 har forsøgt at få en kommentar fra Venstres formand Jakob Ellemann-Jensen for at høre, hvordan han forholder sig til undersøgelsen og Cecilia Lonning-Skovgaards fremtid.']\n",
    "\n",
    "test_corpus = ['I brevet fra fagforbundene lyder opfordringen desuden, at Beskæftigelses- og Integrationsudvalget i Københavns Kommune \"øjeblikkeligt\" skal få styr på det dårlige arbejdsmiljø under borgmesteren.',\n",
    "               'TV 2 forsøger at få en kommentar fra Cecilia Lonning-Skovgaard.', \n",
    "               'Til Politiken siger hun:- Det tager jeg et kæmpe ansvar for, og det har jeg også sagt til medarbejderne her til morgen og undskyldt for. Jeg er i fuld gang med at arbejde på tonen over for medarbejderne, og jeg er også stoppet med at tage direkte kontakt til medarbejderne.',\n",
    "               'Borgmesteren har tidligere oplyst, at hun har søgt professionel hjælp til at forbedre sin ledelsesstil.',\n",
    "               'Undersøgelsen af det psykiske arbejdsmiljø blev igangsat i januar, efter at HK og DJØF i et åbent brev kritiserede Cecilia Lonning-Skovgaard for manglende indsigt i sin forvaltning og manglende forståelse for sin rolle som borgmester.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    return re.sub(r',|\\.|:|!|\\?|;', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_ngrams(text, n): \n",
    "#     no_punct_text = remove_punctuation(text)\n",
    "    \n",
    "#     #create ngrams of words\n",
    "#     word_ngrams = ngrams(no_punct_text.split(), n)\n",
    "#     ngram_list = []\n",
    "#     for ngram in word_ngrams: \n",
    "#         ngram_list.append(ngram)\n",
    "    \n",
    "#     #create character ngrams\n",
    "#     no_space_text = no_punct_text.replace(\" \",\"\")\n",
    "#     char_list = list(no_space_text.lower())\n",
    "#     char_ngrams = ngrams(char_list, n)\n",
    "    \n",
    "#     char_ngram_list = []\n",
    "#     for char_ngram in char_ngrams: \n",
    "#         char_ngram_list.append(char_ngram)\n",
    "    \n",
    "#     output = {\"ngrams\": ngram_list, \n",
    "#               \"char_ngrams\": char_ngram_list}\n",
    "    \n",
    "#     return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#POS tagger trained on Danish news and media corpus\n",
    "POS_tagger_DK = spacy.load(\"da_core_news_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trains a TF-IDF vectorizer of word n-grams\n",
    "def word_ngram_vectorizer(train_corpus, n): \n",
    "    \n",
    "    vectorizer = TfidfVectorizer(analyzer=\"word\", ngram_range=(n,n))\n",
    "    X = vectorizer.fit_transform(train_corpus)\n",
    "    ngrams = vectorizer.get_feature_names()\n",
    "    dense = X.todense()\n",
    "    denselist = dense.tolist()\n",
    "    df = pd.DataFrame(denselist, columns=ngrams)\n",
    "    display(df.head(2))\n",
    "      \n",
    "    return X, vectorizer\n",
    "\n",
    "#Trains a TF-IDF vectorizer of character n-grams\n",
    "def char_ngram_vectorizer(train_corpus, n): \n",
    "    \n",
    "    vectorizer = TfidfVectorizer(analyzer=\"char\", ngram_range=(n,n))\n",
    "    X = vectorizer.fit_transform(train_corpus)\n",
    "    ngrams = vectorizer.get_feature_names()\n",
    "    dense = X.todense()\n",
    "    denselist = dense.tolist()\n",
    "    df = pd.DataFrame(denselist, columns=ngrams)\n",
    "    display(df.head(2))\n",
    "      \n",
    "    return X, vectorizer\n",
    "\n",
    "#Trains a TF-IDF vectorizer of POS n-grams. A POS corpus is generated in the function using a tagger for Danish\n",
    "def POS_ngram_vectorizer(train_corpus, n): \n",
    "    \n",
    "    #Create POS corpus\n",
    "    POS_corpus = []\n",
    "\n",
    "    for doc in train_corpus:\n",
    "        tagged_doc = POS_tagger_DK(doc) #tag each document in corpus with POS tags using spacy\n",
    "        POS_list = []\n",
    "\n",
    "        for token in tagged_doc:\n",
    "            POS_list.append(token.pos_)\n",
    "\n",
    "        #concatenate as POS tags for the document\n",
    "        POS_text = \" \".join(POS_list)\n",
    "        POS_corpus.append(POS_text)\n",
    "\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(analyzer=\"word\", ngram_range=(n,n))\n",
    "    X = vectorizer.fit_transform(POS_corpus)\n",
    "    ngrams = vectorizer.get_feature_names()\n",
    "    dense = X.todense()\n",
    "    denselist = dense.tolist()\n",
    "    df = pd.DataFrame(denselist, columns=ngrams)\n",
    "    display(df.head(2))\n",
    "    \n",
    "    # Returns \n",
    "    return X, vectorizer\n",
    "\n",
    "# Gets weights for terms based on trained vectorizer\n",
    "# Works for both word and character ngrams\n",
    "def get_tfidf_ngrams(vectorizer, test_corpus):\n",
    "    '''Returns the TF-IDF weighted ngram frequencies of test documents'''\n",
    "    #Multiple texts required\n",
    "    return vectorizer.transform(test_corpus)\n",
    "\n",
    "# Function generates POS test corpus first and then gts weights for terms based on trained vectorizer. \n",
    "def get_tfidf_POS_ngrams(vectorizer, test_corpus):\n",
    "    '''Returns the TF-IDF weighted ngram frequencies of test documents'''\n",
    "    #Create POS corpus\n",
    "    POS_corpus = []\n",
    "\n",
    "    for doc in test_corpus:\n",
    "        tagged_doc = POS_tagger_DK(doc) #tag each document in corpus with POS tags using spacy\n",
    "        POS_list = []\n",
    "\n",
    "        for token in tagged_doc:\n",
    "            POS_list.append(token.pos_)\n",
    "\n",
    "        #concatenate as POS tags for the document\n",
    "        POS_text = \" \".join(POS_list)\n",
    "        POS_corpus.append(POS_text)\n",
    "    \n",
    "    #Multiple texts required\n",
    "    return vectorizer.transform(POS_corpus)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adj adp part</th>\n",
       "      <th>adj noun adp</th>\n",
       "      <th>adj noun propn</th>\n",
       "      <th>adj noun punct</th>\n",
       "      <th>adp adj noun</th>\n",
       "      <th>adp adv punct</th>\n",
       "      <th>adp det adj</th>\n",
       "      <th>adp noun cconj</th>\n",
       "      <th>adp noun noun</th>\n",
       "      <th>adp noun num</th>\n",
       "      <th>...</th>\n",
       "      <th>verb adj noun</th>\n",
       "      <th>verb adp adv</th>\n",
       "      <th>verb adp det</th>\n",
       "      <th>verb det adj</th>\n",
       "      <th>verb det noun</th>\n",
       "      <th>verb part verb</th>\n",
       "      <th>verb pron adp</th>\n",
       "      <th>verb propn propn</th>\n",
       "      <th>verb punct adv</th>\n",
       "      <th>verb punct sconj</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.205575</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.156345</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.205575</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.205575</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.205575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.148417</td>\n",
       "      <td>0.296834</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296834</td>\n",
       "      <td>0.148417</td>\n",
       "      <td>0.148417</td>\n",
       "      <td>0.112875</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.148417</td>\n",
       "      <td>0.148417</td>\n",
       "      <td>0.148417</td>\n",
       "      <td>0.148417</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 73 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   adj adp part  adj noun adp  adj noun propn  adj noun punct  adp adj noun  \\\n",
       "0      0.000000      0.000000        0.205575        0.000000      0.000000   \n",
       "1      0.148417      0.296834        0.000000        0.296834      0.148417   \n",
       "\n",
       "   adp adv punct  adp det adj  adp noun cconj  adp noun noun  adp noun num  \\\n",
       "0       0.000000     0.156345             0.0            0.0      0.205575   \n",
       "1       0.148417     0.112875             0.0            0.0      0.000000   \n",
       "\n",
       "   ...  verb adj noun  verb adp adv  verb adp det  verb det adj  \\\n",
       "0  ...       0.000000      0.000000      0.000000      0.000000   \n",
       "1  ...       0.148417      0.148417      0.148417      0.148417   \n",
       "\n",
       "   verb det noun  verb part verb  verb pron adp  verb propn propn  \\\n",
       "0            0.0             0.0            0.0          0.205575   \n",
       "1            0.0             0.0            0.0          0.000000   \n",
       "\n",
       "   verb punct adv  verb punct sconj  \n",
       "0             0.0          0.205575  \n",
       "1             0.0          0.000000  \n",
       "\n",
       "[2 rows x 73 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X, POS_trigram_vectorizer = POS_ngram_vectorizer(train_corpus,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.40387946, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.30716074, 0.        , 0.        , 0.        ,\n",
       "        0.30716074, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.30716074, 0.        , 0.        , 0.40387946, 0.47707544,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.40387946, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.32200242, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.42339448,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.42339448, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.42339448, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.42339448, 0.42339448, 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.55183598, 0.        , 0.        , 0.18394533,\n",
       "        0.        , 0.        , 0.18394533, 0.        , 0.        ,\n",
       "        0.27979033, 0.27979033, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.36789066, 0.        ,\n",
       "        0.13989516, 0.        , 0.        , 0.        , 0.10864107,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.18394533, 0.        , 0.        ,\n",
       "        0.        , 0.18394533, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.18394533, 0.        , 0.        , 0.        ,\n",
       "        0.18394533, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.36789066, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.18394533, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.38988801, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.29651988, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.38988801, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.38988801, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.38988801, 0.        ,\n",
       "        0.        , 0.        , 0.38988801, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.38988801],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.3252664 , 0.        , 0.        , 0.        ,\n",
       "        0.3252664 , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.21384311, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.3252664 , 0.1626332 , 0.        , 0.64152934, 0.12629919,\n",
       "        0.21384311, 0.21384311, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.21384311,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.21384311, 0.        , 0.        ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "POS_trigram_vector = get_tfidf_POS_ngrams(POS_trigram_vectorizer, test_corpus).toarray()\n",
    "POS_trigram_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
