{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here we go again..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simpletransformers.language_representation import RepresentationModel\n",
    "from simpletransformers.config.model_args import ModelArgs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data\n",
    "\n",
    "When running the function, input the path to where the desired dataset(s) is/are located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path): #input the path to the directory with data\n",
    "    frames = []\n",
    "    \n",
    "    _, _, files = next(os.walk(path)) #create a list of all datafile names     \n",
    "          \n",
    "    for file in tqdm(files): #for every file in directory\n",
    "        with open(path+\"/\"+file) as f: #read each file\n",
    "            dataframe = pd.read_json(f) #convert file to dataframe\n",
    "     \n",
    "        frames.append(dataframe) #append each dataframe to list\n",
    "    data = pd.concat(frames, sort=False) #make it one big dataframe\n",
    "    \n",
    "    return data, frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_subset, author_subset_df_list = read_data(\"final_subsets/final_author_subset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_subset, domain_subset_df_list = read_data(\"final_subsets/final_domain_subset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding entire dataset\n",
    "\n",
    "When running the function that encodes the dataset, make sure the dataset is formatted as a list of dataframes - the 2nd object that's returned from the ```read_data``` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_symbols(text):\n",
    "    return re.sub('\\W+',' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_dataset(bodies, target, file_name=\"train\", subset=\"authors\", training_epochs=1): #data = bodies of dataset, target = target values, i.e. domains or authors\n",
    "      \n",
    "    model_args = ModelArgs(encoding=\"utf-8\", manual_seed=42, num_train_epochs=training_epochs)\n",
    "\n",
    "    print(\"Initializing Representation Model\")\n",
    "    model = RepresentationModel(\n",
    "                model_type='bert',\n",
    "                model_name='Maltehb/danish-bert-botxo',\n",
    "                args=model_args,\n",
    "                use_cuda=False)\n",
    "\n",
    "    lower_bodies = []\n",
    "\n",
    "    #clean bodies from punctuation and lowercase words\n",
    "    for text in bodies: \n",
    "        text = text.lower()\n",
    "        lower_bodies.append(text)\n",
    "\n",
    "    #encode lowered bodies\n",
    "    print(f\"Encoding {file_name}set for {subset} subset...\")\n",
    "    word_vectors = model.encode_sentences(lower_bodies, combine_strategy='mean') \n",
    "\n",
    "    if subset == \"authors\":\n",
    "        \n",
    "        np.save(f\"auto_encodings/author_encodings/{file_name}_X\", word_vectors) \n",
    "        np.save(f\"auto_encodings/author_encodings/{file_name}_y\", target)\n",
    "    \n",
    "    if subset == \"domains\":\n",
    "        \n",
    "        np.save(f\"auto_encodings/domain_encodings/{file_name}_X\", word_vectors) \n",
    "        np.save(f\"auto_encodings/domain_encodings/{file_name}_y\", target)\n",
    "    \n",
    "    \n",
    "    #save encodings to file numbered with matching index of current datafile\n",
    "    np.save(f\"auto_encodings/{file_name}_autoencodings_{subset}\", word_vectors) \n",
    "    \n",
    "    np.save(f\"auto_encodings/{file_name}_autoencodings_{subset}_target\", target)\n",
    "        \n",
    "    print(\"Data saved o/\\o\")\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_bodies = list(author_subset.Body)\n",
    "authors = list(author_subset.Byline)\n",
    "\n",
    "domain_bodies = list(author_subset.Body)\n",
    "domains = list(author_subset.Byline)\n",
    "\n",
    "bodies_author_sub = auhthor_bodies[:500]\n",
    "authors_sub = authors[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_authors, test_X_authors, train_y_authors, test_y_authors = train_test_split(author_bodies, authors, test_size=0.2, random_state=42, stratify=authors)\n",
    "train_X_domains, test_X_domains, train_y_domains, test_y_domains = train_test_split(domain_bodies, domains, test_size=0.2, random_state=42, stratify=domains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_X_authors), len(test_X_authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "encode_dataset(train_X_authors, train_y_authors, file_name=\"train\", subset=\"authors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "encode_dataset(test_X_authors, test_y_authors, file_name=\"test\", subset=\"authors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "encode_dataset(train_X_domains, train_y_domains, file_name=\"train\", subset=\"domains\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "encode_dataset(train_X_domains, train_y_domains, file_name=\"test\", subset=\"domains\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading encodings from saved files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_autoencodings(path, train_or_test=\"train\"): \n",
    "    \n",
    "    _,_, files = next(os.walk(path))\n",
    "\n",
    "    encodings_array = []\n",
    "    target_array = []\n",
    "    \n",
    "    for file in tqdm(files):\n",
    "\n",
    "        if file == f\"{train_or_test}_X.npy\":\n",
    "            encodings = np.load(path+'/'+file)\n",
    "            encodings_array.append(encodings)\n",
    "            \n",
    "        if file == f\"{train_or_test}_y.npy\": \n",
    "            target = np.load(path+'/'+file)\n",
    "            target_array.append(target)\n",
    "                \n",
    "\n",
    "    return encodings, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_X_train, author_y_train = load_autoencodings(\"auto_encodings/author_encodings\", train_or_test=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_X_test, author_y_test = load_autoencodings(\"auto_encodings/author_encodings\", train_or_test=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_X_train, domain_y_train = load_autoencodings(\"auto_encodings/domain_encodings\", train_or_test=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_X_test, domain_y_test = load_autoencodings(\"auto_encodings/domain_encodings\", train_or_test=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [0]*100 + [1]*100 + [2]*100 + [3]*100 + [4]*100 + [5]*100 + [6]*100\n",
    "# y = [0]*1000 + [1]*1000 + [2]*1000 + [3]*1000 + [4]*1000 + [5]*1000 +[6]*1000\n",
    "\n",
    "# train_X, test_X, train_y, test_y = train_test_split(encodings, domains)\n",
    "# test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier().fit(train_encodings, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc.score(test_encodings, test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import *\n",
    "\n",
    "dum = DummyClassifier().fit(train_encodings, train_target)\n",
    "dum.score(test_encodings, test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import classifier_unit_test\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpr_fpr = classifier_unit_test.test_classifier(rfc, train_X, test_X, train_y, test_y, give_roc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_pos_rate = tpr_fpr[\"tpr\"]\n",
    "false_pos_rate = tpr_fpr[\"fpr\"]\n",
    "dum_tpr = tpr_fpr[\"dum_tpr\"]\n",
    "dum_fpr = tpr_fpr[\"dum_fpr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(false_pos_rate, true_pos_rate)\n",
    "plt.plot(dum_tpr,dum_fpr)\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the shit on codified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codified = np.load('../../../codified.npy')\n",
    "codified.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = list(data.Header.astype(str))\n",
    "texts = text\n",
    "final_text = ''\n",
    "for text in tqdm(texts):\n",
    "    text = text.strip()\n",
    "    text = text.replace('\\n','')\n",
    "    text = text.replace('\\r','')\n",
    "    text = text.replace('\\t','')\n",
    "    if len(text) < 10:\n",
    "        continue\n",
    "    final_text += ''.join(text)\n",
    "    final_text += '\\n'\n",
    "final_text = final_text[:-1]\n",
    "\n",
    "headers = final_text.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "politiken2_ind = []\n",
    "information2_ind = []\n",
    "politiken2 = []\n",
    "information2 = []\n",
    "\n",
    "for i, header in tqdm(enumerate(headers)):\n",
    "    if header in politiken:\n",
    "        politiken2_ind.append(i)\n",
    "    elif header in information:\n",
    "        information2_ind.append(i)\n",
    "    \n",
    "    if len(politiken2) == 1000 and len(information2)==1000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "politiken2_vecs = np.array([codified[i] for i in politiken2_ind])\n",
    "information2_vecs = np.array([codified[i] for i in information2_ind][:992])\n",
    "codified_vecs = np.vstack((politiken2_vecs, information2_vecs))\n",
    "\n",
    "y = [0]*992 + [1]*992\n",
    "train_X, test_X, train_y, test_y = train_test_split(codified_vecs, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random.shuffle(y)\n",
    "# train_X, test_X, train_y, test_y = train_test_split(codified_vecs, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import classifier_unit_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_unit_test.test_classifier(rfc, train_X, test_X, train_y, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make vector of some articles\n",
    "bodies = list(data[\"Body\"])[9000:9100]\n",
    "headers = list(data[\"Header\"])[9000:9100]\n",
    "\n",
    "model = RepresentationModel(\n",
    "        model_type='bert',\n",
    "        model_name='Maltehb/danish-bert-botxo',\n",
    "        use_cuda=False)\n",
    "\n",
    "vectors = model.encode_sentences(bodies, combine_strategy='mean')\n",
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [[vectors[i], headers[i]] for i in range(len(headers)-10)]\n",
    "eval_data = [[vectors[i], headers[i]] for i in range(len(headers)-10, len(headers))]\n",
    "train_df = pd.DataFrame(train_data, columns=[\"input\", \"target\"])\n",
    "eval_df = pd.DataFrame(eval_data, columns=[\"input\", \"target\"])\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\")\n",
    "model = MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\")\n",
    "embeddings = model.get_input_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
