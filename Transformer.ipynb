{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here we go again..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-21T11:17:47.317528Z",
     "iopub.status.busy": "2022-05-21T11:17:47.317044Z",
     "iopub.status.idle": "2022-05-21T11:17:50.786348Z",
     "shell.execute_reply": "2022-05-21T11:17:50.785113Z",
     "shell.execute_reply.started": "2022-05-21T11:17:47.317475Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from simpletransformers.language_representation import RepresentationModel\n",
    "from simpletransformers.config.model_args import ModelArgs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data\n",
    "\n",
    "When running the function, input the path to where the desired dataset(s) is/are located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-21T11:17:53.984785Z",
     "iopub.status.busy": "2022-05-21T11:17:53.984245Z",
     "iopub.status.idle": "2022-05-21T11:17:53.995604Z",
     "shell.execute_reply": "2022-05-21T11:17:53.994528Z",
     "shell.execute_reply.started": "2022-05-21T11:17:53.984727Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_data(path): #input the path to the directory with data\n",
    "    frames = []\n",
    "    \n",
    "    _, _, files = next(os.walk(path)) #create a list of all datafile names     \n",
    "          \n",
    "    for file in tqdm(files): #for every file in directory\n",
    "        with open(path+\"/\"+file) as f: #read each file\n",
    "            dataframe = pd.read_json(f) #convert file to dataframe\n",
    "     \n",
    "        frames.append(dataframe) #append each dataframe to list\n",
    "    data = pd.concat(frames, sort=False) #make it one big dataframe\n",
    "    \n",
    "    return data, frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T12:24:56.208862Z",
     "iopub.status.busy": "2022-05-18T12:24:56.208143Z",
     "iopub.status.idle": "2022-05-18T12:24:59.013573Z",
     "shell.execute_reply": "2022-05-18T12:24:59.012637Z",
     "shell.execute_reply.started": "2022-05-18T12:24:56.208800Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d948f49684e941208ae3a6c10f25f649",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "author_subset, author_subset_df_list = read_data(\"Data/xtra_clean_authors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T12:24:59.120717Z",
     "iopub.status.busy": "2022-05-18T12:24:59.120594Z",
     "iopub.status.idle": "2022-05-18T12:25:01.123203Z",
     "shell.execute_reply": "2022-05-18T12:25:01.122357Z",
     "shell.execute_reply.started": "2022-05-18T12:24:59.120701Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce8f76097848484f9310d592742dfcc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "domain_subset, domain_subset_df_list = read_data(\"Data/xtra_clean_domains\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-21T11:18:01.930188Z",
     "iopub.status.busy": "2022-05-21T11:18:01.929606Z",
     "iopub.status.idle": "2022-05-21T11:18:12.597178Z",
     "shell.execute_reply": "2022-05-21T11:18:12.595261Z",
     "shell.execute_reply.started": "2022-05-21T11:18:01.930125Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c594df15793439faf3f7d4560072f5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "header_subset, header_subset_df_list = read_data(\"Data/xtra_clean_headlines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-21T11:18:12.782680Z",
     "iopub.status.busy": "2022-05-21T11:18:12.782553Z",
     "iopub.status.idle": "2022-05-21T11:18:12.788667Z",
     "shell.execute_reply": "2022-05-21T11:18:12.787973Z",
     "shell.execute_reply.started": "2022-05-21T11:18:12.782664Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(214921, 10)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header_subset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding entire dataset\n",
    "\n",
    "When running the function that encodes the dataset, make sure the dataset is formatted as a list of dataframes - the 2nd object that's returned from the ```read_data``` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-21T11:18:54.179973Z",
     "iopub.status.busy": "2022-05-21T11:18:54.179258Z",
     "iopub.status.idle": "2022-05-21T11:18:54.194227Z",
     "shell.execute_reply": "2022-05-21T11:18:54.193419Z",
     "shell.execute_reply.started": "2022-05-21T11:18:54.179881Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def encode_dataset(bodies, target, file_name=\"train\", subset=\"authors\", training_epochs=1): #data = bodies of dataset, target = target values, i.e. domains or authors\n",
    "      \n",
    "    model_args = ModelArgs(encoding=\"utf-8\", manual_seed=42, num_train_epochs=training_epochs)\n",
    "\n",
    "    print(\"Initializing Representation Model\")\n",
    "    model = RepresentationModel(\n",
    "                model_type='bert',\n",
    "                model_name='Maltehb/danish-bert-botxo',\n",
    "                args=model_args,\n",
    "                use_cuda=False)\n",
    "\n",
    "    lower_bodies = []\n",
    "\n",
    "    #clean bodies from punctuation and lowercase words\n",
    "    for text in bodies: \n",
    "        text = text.lower()\n",
    "        lower_bodies.append(text)\n",
    "\n",
    "    #encode lowered bodies\n",
    "    print(f\"Encoding {file_name}set for {subset} subset...\")\n",
    "    word_vectors = model.encode_sentences(lower_bodies, combine_strategy='mean') \n",
    "\n",
    "    if subset == \"authors\":\n",
    "        \n",
    "        np.save(f\"auto_encodings/xtra_author_encodings/{file_name}_X\", word_vectors) \n",
    "        np.save(f\"auto_encodings/xtra_author_encodings/{file_name}_y\", target)\n",
    "    \n",
    "    if subset == \"domains\":\n",
    "        \n",
    "        np.save(f\"auto_encodings/xtra_domain_encodings/{file_name}_X\", word_vectors) \n",
    "        np.save(f\"auto_encodings/xtra_domain_encodings/{file_name}_y\", target)\n",
    "    \n",
    "    if subset == \"headers\":\n",
    "        \n",
    "        np.save(f\"auto_encodings/xtra_header_encodings/{file_name}_X\", word_vectors) \n",
    "        np.save(f\"auto_encodings/xtra_header_encodings/{file_name}_y\", target)\n",
    "\n",
    "        \n",
    "    print(\"Data saved o/\\o\")\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-21T11:18:55.551847Z",
     "iopub.status.busy": "2022-05-21T11:18:55.551269Z",
     "iopub.status.idle": "2022-05-21T11:18:56.164116Z",
     "shell.execute_reply": "2022-05-21T11:18:56.163356Z",
     "shell.execute_reply.started": "2022-05-21T11:18:55.551789Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-21T13:05:07.348855Z",
     "iopub.status.busy": "2022-05-21T13:05:07.348347Z",
     "iopub.status.idle": "2022-05-21T13:05:07.458300Z",
     "shell.execute_reply": "2022-05-21T13:05:07.457685Z",
     "shell.execute_reply.started": "2022-05-21T13:05:07.348800Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# author_bodies = list(author_subset.Body)\n",
    "# authors = list(author_subset.Byline)\n",
    "\n",
    "# domain_bodies = list(domain_subset.Body)\n",
    "# domains = list(domain_subset.Domain)\n",
    "\n",
    "header_bodies = list(header_subset.Body)\n",
    "headers = list(header_subset.Header)\n",
    "\n",
    "# bodies_author_sub = author_bodies[:500]\n",
    "# authors_sub = authors[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-21T11:20:13.171075Z",
     "iopub.status.busy": "2022-05-21T11:20:13.170774Z",
     "iopub.status.idle": "2022-05-21T11:20:13.175651Z",
     "shell.execute_reply": "2022-05-21T11:20:13.175099Z",
     "shell.execute_reply.started": "2022-05-21T11:20:13.171056Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "214921"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(header_bodies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-21T13:05:13.493340Z",
     "iopub.status.busy": "2022-05-21T13:05:13.492815Z",
     "iopub.status.idle": "2022-05-21T13:05:13.518220Z",
     "shell.execute_reply": "2022-05-21T13:05:13.517479Z",
     "shell.execute_reply.started": "2022-05-21T13:05:13.493287Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'author_bodies' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_X_authors, test_X_authors, train_y_authors, test_y_authors \u001b[38;5;241m=\u001b[39m train_test_split(\u001b[43mauthor_bodies\u001b[49m, authors, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, stratify\u001b[38;5;241m=\u001b[39mauthors)\n\u001b[1;32m      2\u001b[0m train_X_domains, test_X_domains, train_y_domains, test_y_domains \u001b[38;5;241m=\u001b[39m train_test_split(domain_bodies, domains, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, stratify\u001b[38;5;241m=\u001b[39mdomains)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'author_bodies' is not defined"
     ]
    }
   ],
   "source": [
    "train_X_authors, test_X_authors, train_y_authors, test_y_authors = train_test_split(author_bodies, authors, test_size=0.2, random_state=42, stratify=authors)\n",
    "train_X_domains, test_X_domains, train_y_domains, test_y_domains = train_test_split(domain_bodies, domains, test_size=0.2, random_state=42, stratify=domains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-21T13:06:31.971107Z",
     "iopub.status.busy": "2022-05-21T13:06:31.970588Z",
     "iopub.status.idle": "2022-05-21T13:06:32.120208Z",
     "shell.execute_reply": "2022-05-21T13:06:32.119559Z",
     "shell.execute_reply.started": "2022-05-21T13:06:31.971051Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_X_headers, test_X_headers, train_y_headers, test_y_headers = train_test_split(header_bodies, headers, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-21T13:06:06.479262Z",
     "iopub.status.busy": "2022-05-21T13:06:06.479102Z",
     "iopub.status.idle": "2022-05-21T13:06:06.509701Z",
     "shell.execute_reply": "2022-05-21T13:06:06.508885Z",
     "shell.execute_reply.started": "2022-05-21T13:06:06.479242Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_X_domains' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mlen\u001b[39m(\u001b[43mtrain_X_domains\u001b[49m), \u001b[38;5;28mlen\u001b[39m(test_X_domains)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_X_domains' is not defined"
     ]
    }
   ],
   "source": [
    "len(train_X_domains), len(test_X_domains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-21T13:06:52.814266Z",
     "iopub.status.busy": "2022-05-21T13:06:52.813748Z",
     "iopub.status.idle": "2022-05-21T13:06:52.822774Z",
     "shell.execute_reply": "2022-05-21T13:06:52.821738Z",
     "shell.execute_reply.started": "2022-05-21T13:06:52.814210Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'13-årig kørt ned, da han løb over for rødt lys'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-21T13:07:01.801634Z",
     "iopub.status.busy": "2022-05-21T13:07:01.801135Z",
     "iopub.status.idle": "2022-05-21T14:07:42.113217Z",
     "shell.execute_reply": "2022-05-21T14:07:42.111657Z",
     "shell.execute_reply.started": "2022-05-21T13:07:01.801581Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Representation Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Maltehb/danish-bert-botxo were not used when initializing BertForTextRepresentation: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForTextRepresentation from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTextRepresentation from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding trainset for headers subset...\n",
      "Data saved o/\\o\n",
      "CPU times: user 20h 45min 58s, sys: 1h 2min 6s, total: 21h 48min 4s\n",
      "Wall time: 1h 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "encode_dataset(train_X_headers, train_y_headers, file_name=\"train\", subset=\"headers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-21T14:08:33.789525Z",
     "iopub.status.busy": "2022-05-21T14:08:33.789302Z",
     "iopub.status.idle": "2022-05-21T14:23:52.523919Z",
     "shell.execute_reply": "2022-05-21T14:23:52.523154Z",
     "shell.execute_reply.started": "2022-05-21T14:08:33.789505Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Representation Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Maltehb/danish-bert-botxo were not used when initializing BertForTextRepresentation: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForTextRepresentation from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTextRepresentation from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding testset for headers subset...\n",
      "Data saved o/\\o\n",
      "CPU times: user 5h 21min 36s, sys: 6min 23s, total: 5h 28min\n",
      "Wall time: 15min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "encode_dataset(test_X_headers, test_y_headers, file_name=\"test\", subset=\"headers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-21T14:44:28.278365Z",
     "iopub.status.busy": "2022-05-21T14:44:28.277810Z",
     "iopub.status.idle": "2022-05-21T14:44:28.456088Z",
     "shell.execute_reply": "2022-05-21T14:44:28.455366Z",
     "shell.execute_reply.started": "2022-05-21T14:44:28.278309Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "f = np.load('auto_encodings/xtra_header_encodings/train_y.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-21T14:45:18.762286Z",
     "iopub.status.busy": "2022-05-21T14:45:18.762099Z",
     "iopub.status.idle": "2022-05-21T14:45:18.770340Z",
     "shell.execute_reply": "2022-05-21T14:45:18.769275Z",
     "shell.execute_reply.started": "2022-05-21T14:45:18.762264Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Jubler over dansk trøffelfund: Tre gange større end dem i bøgerne - TV 2',\n",
       "       'Greta Thunberg har fået et dyr opkaldt efter sig',\n",
       "       'WiFi-frygt: 1.326 fly skal have skiftet cockpit-skærme', ...,\n",
       "       'Københavns Kommune stopper ulovlig nedrivning af bevaringsværdig skorsten',\n",
       "       'Analyse: Kaptajn Schettiono er en nyttig idiot og antihelt',\n",
       "       'Tove Los feministiske dancefloor-agenda drukner i flade poptrends'],\n",
       "      dtype='<U454')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T12:28:27.896211Z",
     "iopub.status.busy": "2022-05-18T12:28:27.896051Z",
     "iopub.status.idle": "2022-05-18T12:28:27.899740Z",
     "shell.execute_reply": "2022-05-18T12:28:27.899217Z",
     "shell.execute_reply.started": "2022-05-18T12:28:27.896189Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# encode_dataset(train_X_authors, train_y_authors, file_name=\"train\", subset=\"authors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T12:28:27.900610Z",
     "iopub.status.busy": "2022-05-18T12:28:27.900422Z",
     "iopub.status.idle": "2022-05-18T12:28:27.947267Z",
     "shell.execute_reply": "2022-05-18T12:28:27.946155Z",
     "shell.execute_reply.started": "2022-05-18T12:28:27.900592Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# encode_dataset(test_X_authors, test_y_authors, file_name=\"test\", subset=\"authors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T12:28:27.950568Z",
     "iopub.status.busy": "2022-05-18T12:28:27.950061Z",
     "iopub.status.idle": "2022-05-18T12:58:32.026056Z",
     "shell.execute_reply": "2022-05-18T12:58:32.025098Z",
     "shell.execute_reply.started": "2022-05-18T12:28:27.950516Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Representation Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Maltehb/danish-bert-botxo were not used when initializing BertForTextRepresentation: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForTextRepresentation from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTextRepresentation from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding trainset for domains subset...\n",
      "Data saved o/\\o\n",
      "CPU times: user 11h 15min 32s, sys: 30min 55s, total: 11h 46min 27s\n",
      "Wall time: 30min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "encode_dataset(train_X_domains, train_y_domains, file_name=\"train\", subset=\"domains\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T12:58:32.027674Z",
     "iopub.status.busy": "2022-05-18T12:58:32.027513Z",
     "iopub.status.idle": "2022-05-18T13:06:10.463057Z",
     "shell.execute_reply": "2022-05-18T13:06:10.461432Z",
     "shell.execute_reply.started": "2022-05-18T12:58:32.027652Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Representation Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Maltehb/danish-bert-botxo were not used when initializing BertForTextRepresentation: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForTextRepresentation from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTextRepresentation from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding testset for domains subset...\n",
      "Data saved o/\\o\n",
      "CPU times: user 2h 55min 18s, sys: 3min 16s, total: 2h 58min 35s\n",
      "Wall time: 7min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "encode_dataset(test_X_domains, test_y_domains, file_name=\"test\", subset=\"domains\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading encodings from saved files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_autoencodings(path, train_or_test=\"train\"): \n",
    "    \n",
    "    _,_, files = next(os.walk(path))\n",
    "\n",
    "    encodings_array = []\n",
    "    target_array = []\n",
    "    \n",
    "    for file in tqdm(files):\n",
    "\n",
    "        if file == f\"{train_or_test}_X.npy\":\n",
    "            encodings = np.load(path+'/'+file)\n",
    "            encodings_array.append(encodings)\n",
    "            \n",
    "        if file == f\"{train_or_test}_y.npy\": \n",
    "            target = np.load(path+'/'+file)\n",
    "            target_array.append(target)\n",
    "                \n",
    "\n",
    "    return encodings, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_X_train, author_y_train = load_autoencodings(\"auto_encodings/author_encodings\", train_or_test=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_X_test, author_y_test = load_autoencodings(\"auto_encodings/author_encodings\", train_or_test=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_X_train, domain_y_train = load_autoencodings(\"auto_encodings/domain_encodings\", train_or_test=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_X_test, domain_y_test = load_autoencodings(\"auto_encodings/domain_encodings\", train_or_test=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [0]*100 + [1]*100 + [2]*100 + [3]*100 + [4]*100 + [5]*100 + [6]*100\n",
    "# y = [0]*1000 + [1]*1000 + [2]*1000 + [3]*1000 + [4]*1000 + [5]*1000 +[6]*1000\n",
    "\n",
    "# train_X, test_X, train_y, test_y = train_test_split(encodings, domains)\n",
    "# test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier().fit(train_encodings, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc.score(test_encodings, test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import *\n",
    "\n",
    "dum = DummyClassifier().fit(train_encodings, train_target)\n",
    "dum.score(test_encodings, test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import classifier_unit_test\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpr_fpr = classifier_unit_test.test_classifier(rfc, train_X, test_X, train_y, test_y, give_roc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_pos_rate = tpr_fpr[\"tpr\"]\n",
    "false_pos_rate = tpr_fpr[\"fpr\"]\n",
    "dum_tpr = tpr_fpr[\"dum_tpr\"]\n",
    "dum_fpr = tpr_fpr[\"dum_fpr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(false_pos_rate, true_pos_rate)\n",
    "plt.plot(dum_tpr,dum_fpr)\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the shit on codified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codified = np.load('../../../codified.npy')\n",
    "codified.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = list(data.Header.astype(str))\n",
    "texts = text\n",
    "final_text = ''\n",
    "for text in tqdm(texts):\n",
    "    text = text.strip()\n",
    "    text = text.replace('\\n','')\n",
    "    text = text.replace('\\r','')\n",
    "    text = text.replace('\\t','')\n",
    "    if len(text) < 10:\n",
    "        continue\n",
    "    final_text += ''.join(text)\n",
    "    final_text += '\\n'\n",
    "final_text = final_text[:-1]\n",
    "\n",
    "headers = final_text.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "politiken2_ind = []\n",
    "information2_ind = []\n",
    "politiken2 = []\n",
    "information2 = []\n",
    "\n",
    "for i, header in tqdm(enumerate(headers)):\n",
    "    if header in politiken:\n",
    "        politiken2_ind.append(i)\n",
    "    elif header in information:\n",
    "        information2_ind.append(i)\n",
    "    \n",
    "    if len(politiken2) == 1000 and len(information2)==1000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "politiken2_vecs = np.array([codified[i] for i in politiken2_ind])\n",
    "information2_vecs = np.array([codified[i] for i in information2_ind][:992])\n",
    "codified_vecs = np.vstack((politiken2_vecs, information2_vecs))\n",
    "\n",
    "y = [0]*992 + [1]*992\n",
    "train_X, test_X, train_y, test_y = train_test_split(codified_vecs, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random.shuffle(y)\n",
    "# train_X, test_X, train_y, test_y = train_test_split(codified_vecs, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import classifier_unit_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_unit_test.test_classifier(rfc, train_X, test_X, train_y, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make vector of some articles\n",
    "bodies = list(data[\"Body\"])[9000:9100]\n",
    "headers = list(data[\"Header\"])[9000:9100]\n",
    "\n",
    "model = RepresentationModel(\n",
    "        model_type='bert',\n",
    "        model_name='Maltehb/danish-bert-botxo',\n",
    "        use_cuda=False)\n",
    "\n",
    "vectors = model.encode_sentences(bodies, combine_strategy='mean')\n",
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [[vectors[i], headers[i]] for i in range(len(headers)-10)]\n",
    "eval_data = [[vectors[i], headers[i]] for i in range(len(headers)-10, len(headers))]\n",
    "train_df = pd.DataFrame(train_data, columns=[\"input\", \"target\"])\n",
    "eval_df = pd.DataFrame(eval_data, columns=[\"input\", \"target\"])\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\")\n",
    "model = MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\")\n",
    "embeddings = model.get_input_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

