{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import spacy \n",
    "import json\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_emot():\n",
    "    e = [\"(o_o)\",\":-)\",\":P\",\":D\",\"x)\",\"ᓚᘏᗢ\",\"╯°□°）╯︵ ┻━┻\",\":)\",\n",
    "         \"*<:-)\",\"^_^\",\"(⌐■_■)\",\"¯\\_(ツ)_/¯\", \"(T_T)\",\":o\",\"OwO\",\n",
    "        \"( ͡❛ ͜ʖ ͡❛)\",\"(̶◉͛‿◉̶)\",\"( ≖.≖)\",\"(ㆆ_ㆆ)\",\"ʕ•́ᴥ•̀ʔっ\",\"( ◡́.◡̀)\",\"(^◡^ )\"]\n",
    "    return random.choice(e)\n",
    "\n",
    "def read_data(path): #input the path to the directory with data\n",
    "    frames = []\n",
    "    \n",
    "    _, _, files = next(os.walk(path)) #create a list of all datafile names     \n",
    "          \n",
    "    for file in tqdm(files): #for every file in directory\n",
    "        with open(path+\"/\"+file) as f: #read each file\n",
    "            dataframe = pd.read_json(f) #convert file to dataframe\n",
    "     \n",
    "        frames.append(dataframe) #append each dataframe to list\n",
    "    data = pd.concat(frames, sort=False) #make it one big dataframe\n",
    "    \n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72d85621d1714f65889c6bc550288555",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=31.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data = read_data(\"scraped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['politiken.dk',\n",
       " 'ekstrabladet.dk',\n",
       " 'computerworld.dk',\n",
       " 'jv.dk',\n",
       " 'berlingske.dk',\n",
       " 'jyllands-posten.dk',\n",
       " 'bold.dk',\n",
       " 'dr.dk',\n",
       " 'bt.dk',\n",
       " 'stiften.dk',\n",
       " 'soundvenue.com',\n",
       " 'altinget.dk',\n",
       " 'sn.dk',\n",
       " 'ing.dk',\n",
       " 'gaffa.dk',\n",
       " 'tv2.dk',\n",
       " 'finans.dk',\n",
       " 'version2.dk',\n",
       " 'journalisten.dk',\n",
       " 'information.dk',\n",
       " 'fyens.dk',\n",
       " 'nordjyske.dk',\n",
       " 'ekkofilm.dk',\n",
       " 'seoghoer.dk',\n",
       " 'tv2east.dk',\n",
       " 'billedbladet.dk',\n",
       " 'finanswatch.dk',\n",
       " 'fagbladet3f.dk',\n",
       " 'ejendomswatch.dk',\n",
       " 'borsen.dk',\n",
       " 'motormagasinet.dk',\n",
       " 'helsingordagblad.dk',\n",
       " 'fodevarewatch.dk',\n",
       " 'jiyan.dk',\n",
       " 'pov.international',\n",
       " 'samvirke.dk',\n",
       " 'ugeavisen.dk',\n",
       " 'skalvilege.nu',\n",
       " 'kulturmonitor.dk',\n",
       " 'heleherlev.dk',\n",
       " 'videnskab.dk',\n",
       " 'mobilabonnement.dk',\n",
       " 'bilsektionen.dk',\n",
       " 'dsr.dk',\n",
       " 'wood-supply.dk',\n",
       " 'amtsavisen.dk',\n",
       " 'turisme.nu',\n",
       " 'herningfolkeblad.dk',\n",
       " 'prosa.dk',\n",
       " 'aktieraadet.dk',\n",
       " 'alt.dk',\n",
       " 'altomvarebiler.dk',\n",
       " 'betxpert.com',\n",
       " 'finansbureauet.dk',\n",
       " 'tv2lorry.dk',\n",
       " 'nordea.com',\n",
       " 'dagensmedicin.dk',\n",
       " 'itsfashionbaby.dk',\n",
       " 'dfi.dk',\n",
       " 'linebaundanielsen.dk',\n",
       " 'femina.dk',\n",
       " 'kristeligt-dagblad.dk',\n",
       " 'skivefolkeblad.dk',\n",
       " 'vice.com',\n",
       " 'avisendanmark.dk',\n",
       " 'weekendavisen.dk',\n",
       " 'tv2ostjylland.dk',\n",
       " 'historienet.dk',\n",
       " 'woman.dk',\n",
       " 'sl.dk',\n",
       " 'landbrugsavisen.dk',\n",
       " 'fagbladetfoa.dk',\n",
       " 'tv2fyn.dk',\n",
       " 'techradar.com',\n",
       " 'sdu.dk',\n",
       " 'sundhedskommunikation.com',\n",
       " 'kommunen.dk',\n",
       " 'ugeskriftet.dk',\n",
       " 'erhvervplus.dk',\n",
       " 'fanoeposten.dk',\n",
       " 'politikensundhed.dk',\n",
       " 'travservice.dk',\n",
       " 'danskekommuner.dk',\n",
       " 'kommunikationsforum.dk',\n",
       " 'kl.dk',\n",
       " 'foa.dk',\n",
       " 'def.dk',\n",
       " 'bobedre.dk',\n",
       " 'globalnyt.dk',\n",
       " 'dtu.dk',\n",
       " 'folkeskolen.dk',\n",
       " 'nordtinget.dk',\n",
       " 'offentligledelse.dk',\n",
       " 'nummer9.dk',\n",
       " 'tipsbladet.dk',\n",
       " 'mortenhede.dk',\n",
       " 'eventyrsstyrelsen.dk',\n",
       " 'folketidende.dk',\n",
       " 'building-supply.dk',\n",
       " 'soefart.dk',\n",
       " 'goerdetselv.dk',\n",
       " 'fagbladetboligen.dk',\n",
       " 'frederiksbergliv.dk',\n",
       " 'jens-haag.dk',\n",
       " 'ruder7.dk',\n",
       " 'minmandsitalienskekoekken.dk',\n",
       " 'au.dk',\n",
       " 'euroman.dk',\n",
       " 'hk.dk',\n",
       " 'ugebrev.dk',\n",
       " 'hjertingposten.dk',\n",
       " 'djoefbladet.dk',\n",
       " 'teateravisen.dk',\n",
       " 'udeoghjemme.dk',\n",
       " 'hitmedjobbet.dk',\n",
       " 'nnf.dk',\n",
       " 'religion.dk',\n",
       " 'global.techradar.com',\n",
       " 'journalistforbundet.dk',\n",
       " 'licitationen.dk',\n",
       " 'linkedin.com',\n",
       " 'freyaanduin.dk',\n",
       " 'mindfulnessguiden.dk',\n",
       " 'illvid.dk',\n",
       " 'taenk.dk',\n",
       " 'socialraadgiverne.dk',\n",
       " 'dortheoxgren.dk',\n",
       " 'businessdanmark.dk',\n",
       " 'kapwatch.dk',\n",
       " 'bolius.dk',\n",
       " 'faa.dk',\n",
       " 'amwatch.dk',\n",
       " 'shippingwatch.dk',\n",
       " 'itwatch.dk',\n",
       " 'Politiken.dk',\n",
       " 'dagligvarehandlen.dk',\n",
       " 'medwatch.dk',\n",
       " 'nordicwomeninfilm.com',\n",
       " 'rikkefinland.dk',\n",
       " 'gymnasieforskning.dk',\n",
       " 'akademikerbladet.dk',\n",
       " 'costume.dk',\n",
       " 'energiwatch.dk',\n",
       " 'heartbeats.dk',\n",
       " 'fred.dk',\n",
       " 'leadingcapacity.dk',\n",
       " 'xperimedia.dk',\n",
       " 'skoleborn.dk',\n",
       " 'uddannelsesbladet.dk',\n",
       " 'verdensbedstenyheder.dk',\n",
       " 'viborg-folkeblad.dk',\n",
       " 'advokatwatch.dk',\n",
       " 'dpu.au.dk',\n",
       " 'krop-fysik.dk',\n",
       " 'uddannelsesforbundet.dk',\n",
       " 'a4arbejdsliv.dk',\n",
       " 'jobfisk.dk',\n",
       " 'kulsort.dk',\n",
       " 'advokatsamfundet.dk',\n",
       " 'ctwatch.dk']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = data[\"Domain\"].value_counts().index.tolist()\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "\n",
    "domains_of_interest = ['politiken.dk',\n",
    " 'ekstrabladet.dk',\n",
    " 'computerworld.dk',\n",
    " 'finans.dk',\n",
    " 'berlingske.dk',\n",
    " 'fyens.dk',\n",
    " 'dr.dk',\n",
    " 'journalisten.dk',\n",
    " 'tv2.dk',\n",
    " 'jyllands-posten.dk',\n",
    " 'information.dk',\n",
    " 'jv.dk',\n",
    " 'sn.dk',\n",
    " 'bt.dk',\n",
    " 'version2.dk']\n",
    "\n",
    "for domain in domains_of_interest: \n",
    "    train = data.loc[(data[\"Domain\"] == domain)][:1000]\n",
    "    frames.append(train)\n",
    "\n",
    "trainset = pd.concat(frames, sort=False)\n",
    "\n",
    "bodies = list(trainset[\"Body\"])\n",
    "domains = list(trainset[\"Domain\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(domains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, train_labels, test_labels = train_test_split(bodies, domains, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    return re.sub(r',|\\.|:|!|\\?|;', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#POS tagger trained on Danish news and media corpus\n",
    "POS_tagger_DK = spacy.load(\"da_core_news_md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NGram features - word, characters, POS-tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trains a TF-IDF vectorizer of word n-grams\n",
    "def word_ngram_vectorizer(train_corpus, n): \n",
    "    \n",
    "    vectorizer = TfidfVectorizer(max_features=3000, analyzer=\"word\", ngram_range=(n,n))\n",
    "    print(\"training vectorizer...\",rand_emot())\n",
    "    \n",
    "    X = vectorizer.fit_transform(train_corpus)\n",
    "    print(\"vectorizer fit!\", rand_emot())\n",
    "    \n",
    "    ngrams = vectorizer.get_feature_names()\n",
    "      \n",
    "    return X, vectorizer\n",
    "\n",
    "#Trains a TF-IDF vectorizer of character n-grams\n",
    "def char_ngram_vectorizer(train_corpus, n): \n",
    "    \n",
    "    vectorizer = TfidfVectorizer(max_features=3000, analyzer=\"char\", ngram_range=(n,n))\n",
    "    print(\"training vectorizer...\",rand_emot())\n",
    "    \n",
    "    X = vectorizer.fit_transform(train_corpus)\n",
    "    print(\"vectorizer fit!\", rand_emot())\n",
    "    \n",
    "    ngrams = vectorizer.get_feature_names()\n",
    "    \n",
    "    return X, vectorizer\n",
    "\n",
    "#Trains a TF-IDF vectorizer of POS n-grams. A POS corpus is generated in the function using a tagger for Danish\n",
    "def POS_ngram_vectorizer(train_corpus, n): \n",
    "    \n",
    "    #Create POS corpus\n",
    "    POS_corpus = []\n",
    "\n",
    "    for doc in train_corpus:\n",
    "        tagged_doc = POS_tagger_DK(doc) #tag each document in corpus with POS tags using spacy\n",
    "        POS_list = []\n",
    "\n",
    "        for token in tagged_doc:\n",
    "            POS_list.append(token.pos_)\n",
    "\n",
    "        #concatenate as POS tags for the document\n",
    "        POS_text = \" \".join(POS_list)\n",
    "        POS_corpus.append(POS_text)\n",
    "\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(max_features=3000, analyzer=\"word\", ngram_range=(n,n))\n",
    "    print(\"training vectorizer...\",rand_emot())\n",
    "    \n",
    "    X = vectorizer.fit_transform(POS_corpus)\n",
    "    print(\"vectorizer fit!\", rand_emot())\n",
    "    \n",
    "    ngrams = vectorizer.get_feature_names()\n",
    "    \n",
    "    # Returns \n",
    "    return X, vectorizer\n",
    "\n",
    "# Gets weights for terms based on trained vectorizer\n",
    "# Works for both word and character ngrams and skipgrams\n",
    "def get_tfidf_ngrams(vectorizer, test_corpus):\n",
    "    '''Returns the TF-IDF weighted ngram frequencies of test documents'''\n",
    "    #Multiple texts required\n",
    "    return vectorizer.transform(test_corpus)\n",
    "\n",
    "# Function generates POS test corpus first and then gets weights for terms based on trained vectorizer. \n",
    "def get_tfidf_POS_ngrams(vectorizer, test_corpus):\n",
    "    '''Returns the TF-IDF weighted ngram frequencies of test documents'''\n",
    "    #Create POS corpus\n",
    "    POS_corpus = []\n",
    "\n",
    "    for doc in test_corpus:\n",
    "        tagged_doc = POS_tagger_DK(doc) #tag each document in corpus with POS tags using spacy\n",
    "        POS_list = []\n",
    "\n",
    "        for token in tagged_doc:\n",
    "            POS_list.append(token.pos_)\n",
    "\n",
    "        #concatenate POS tags as one string, i.e. the documented represented as the POS tags\n",
    "        POS_text = \" \".join(POS_list)\n",
    "        POS_corpus.append(POS_text)\n",
    "    \n",
    "    #Multiple texts required\n",
    "    return vectorizer.transform(POS_corpus)\n",
    "\n",
    "def get_tfidf(vectorizer, test_corpus):\n",
    "    '''Returns the TF-IDF weighted word frequencies of test documents'''\n",
    "    #Multiple texts required\n",
    "    return vectorizer.transform(test_corpus).toarray()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training vectorizer... :P\n",
      "vectorizer fit! ¯\\_(ツ)_/¯\n",
      "training vectorizer... ( ≖.≖)\n",
      "vectorizer fit! ( ◡́.◡̀)\n",
      "training vectorizer... (T_T)\n",
      "vectorizer fit! (ㆆ_ㆆ)\n",
      "training vectorizer... ʕ•́ᴥ•̀ʔっ\n",
      "vectorizer fit! :-)\n",
      "training vectorizer... :P\n",
      "vectorizer fit! ^_^\n",
      "training vectorizer... ( ≖.≖)\n",
      "vectorizer fit! ʕ•́ᴥ•̀ʔっ\n",
      "training vectorizer... ( ͡❛ ͜ʖ ͡❛)\n",
      "vectorizer fit! x)\n",
      "training vectorizer... :-)\n",
      "vectorizer fit! OwO\n",
      "training vectorizer... OwO\n",
      "vectorizer fit! (^◡^ )\n"
     ]
    }
   ],
   "source": [
    "X_unigrams_train,unigram_vectorizer = word_ngram_vectorizer(train,1)\n",
    "X_char_unigrams_train,char_unigram_vectorizer = char_ngram_vectorizer(train,1)\n",
    "X_POS_unigram_train, POS_unigram_vectorizer = POS_ngram_vectorizer(train,1)\n",
    "\n",
    "X_bigrams_train,bigram_vectorizer = word_ngram_vectorizer(train,2)\n",
    "X_char_bigrams_train,char_bigram_vectorizer = char_ngram_vectorizer(train,2)\n",
    "X_POS_bigram_train, POS_bigram_vectorizer = POS_ngram_vectorizer(train,2)\n",
    "\n",
    "X_trigrams_train,trigram_vectorizer = word_ngram_vectorizer(train,3)\n",
    "X_char_trigrams_train,char_trigram_vectorizer = char_ngram_vectorizer(train,3)\n",
    "X_POS_trigram_train, POS_trigram_vectorizer = POS_ngram_vectorizer(train,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_unigrams_test = get_tfidf_ngrams(unigram_vectorizer, test).toarray()\n",
    "X_char_unigrams_test = get_tfidf_ngrams(char_unigram_vectorizer, test).toarray()\n",
    "X_POS_unigram_test = get_tfidf_POS_ngrams(POS_unigram_vectorizer, test).toarray()\n",
    "\n",
    "X_bigrams_test = get_tfidf_ngrams(bigram_vectorizer, test).toarray()\n",
    "X_char_bigrams_test = get_tfidf_ngrams(char_bigram_vectorizer, test).toarray()\n",
    "X_POS_bigram_test = get_tfidf_POS_ngrams(POS_bigram_vectorizer, test).toarray()\n",
    "\n",
    "X_trigrams_test = get_tfidf_ngrams(trigram_vectorizer, test).toarray()\n",
    "X_char_trigrams_test = get_tfidf_ngrams(char_trigram_vectorizer, test).toarray()\n",
    "X_POS_trigram_test = get_tfidf_POS_ngrams(POS_trigram_vectorizer, test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = hstack((X_unigrams_train,X_bigrams_train))\n",
    "X_train = hstack((X_train,X_trigrams_train))\n",
    "X_train = hstack((X_train,X_char_unigrams_train))\n",
    "X_train = hstack((X_train,X_char_bigrams_train))\n",
    "X_train = hstack((X_train,X_char_trigrams_train))\n",
    "X_train = hstack((X_train,X_POS_unigram_train))\n",
    "X_train = hstack((X_train,X_POS_bigram_train))\n",
    "X_train = hstack((X_train,X_POS_trigram_train))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.hstack((X_unigrams_test,X_bigrams_test))\n",
    "X_test = np.hstack((X_test,X_trigrams_test))\n",
    "X_test = np.hstack((X_test,X_char_unigrams_test))\n",
    "X_test = np.hstack((X_test,X_char_bigrams_test))\n",
    "X_test = np.hstack((X_test,X_char_trigrams_test))\n",
    "X_test = np.hstack((X_test,X_POS_unigram_test))\n",
    "X_test = np.hstack((X_test,X_POS_bigram_test))\n",
    "X_test = np.hstack((X_test,X_POS_trigram_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((225, 15991), (25, 15991))"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('function_words/funktionsord.txt', \"r\", encoding=\"utf-8\") as fw:\n",
    "    func_words = fw.read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, word in enumerate(func_words):\n",
    "    word = remove_punctuation(word)\n",
    "    func_words[i] = word\n",
    "    if \" \" in word:\n",
    "        func_words.remove(word)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_words(trainset, testset, function_words): \n",
    "    \n",
    "    fw_corpus = []\n",
    "    \n",
    "    for document in tqdm(trainset): \n",
    "        fw_document = \"\"\n",
    "        document = remove_punctuation(document)\n",
    "        words = document.split() #split text into words\n",
    "        \n",
    "        for word in words: \n",
    "            if word in function_words:\n",
    "                fw_document = fw_document + word + \" \"\n",
    "        \n",
    "        fw_corpus.append(fw_document)\n",
    "        \n",
    "    \n",
    "    vectorizer = TfidfVectorizer(max_features=3000, analyzer=\"word\")\n",
    "    print(\"training vectorizer...\",rand_emot())\n",
    "    \n",
    "    X = vectorizer.fit_transform(fw_corpus)\n",
    "    print(\"vectorizer fit!\", rand_emot())\n",
    "    \n",
    "    fw_vectorized = get_tfidf(vectorizer, testset)\n",
    "       \n",
    "    return X, fw_vectorized, fw_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa23a0927f624191b3cc0092ee1bad35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=225.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "training vectorizer... :-)\n",
      "vectorizer fit! x)\n"
     ]
    }
   ],
   "source": [
    "X_fw_train, X_fw_test, fw_corpus = function_words(train, test, func_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.hstack((X_train.todense(), X_fw_train.todense()))\n",
    "X_test = np.hstack((X_test, X_fw_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(225, 16129)\n",
      "(25, 16129)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_train.shape), print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse\n",
    "from scipy.sparse import hstack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skipgrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import skipgrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_words = set(stopwords.words(\"danish\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus_without_stopwords =[]\n",
    "\n",
    "# for doc in tqdm(sub_bodies): \n",
    "#     temp_doc = []\n",
    "#     for word in doc.lower().split(): \n",
    "#         if word not in stop_words: \n",
    "#             temp_doc.append(word)\n",
    "#     corpus_without_stopwords.append(\" \".join(temp_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_corpus =[]\n",
    "\n",
    "# for doc in corpus_without_stopwords: \n",
    "#     split_doc = doc.split(\" \")\n",
    "#     new_corpus.append(split_doc)\n",
    "# corpus = new_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2v_model = Word2Vec(corpus, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2v_model.most_similar(\"mand\", topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skipgram_vectorizer(train_corpus, n, k): #k = step (skip) size\n",
    "    \n",
    "    skipgram_corpus = []\n",
    "    \n",
    "    for doc in tqdm(train_corpus): \n",
    "        l = list(skipgrams(doc.split(), n, k))\n",
    "        new_doc = ' '.join([' '.join(x) for x in l]) #concatenate skipgrams as new representation of doc\n",
    "        skipgram_corpus.append(new_doc)\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(max_features=3000, analyzer=\"word\", ngram_range=(n,n))\n",
    "    print(\"training vectorizer...\",rand_emot())\n",
    "    \n",
    "    X = vectorizer.fit_transform(skipgram_corpus)\n",
    "    print(\"vectorizer fit!\", rand_emot())\n",
    "    \n",
    "    ngrams = vectorizer.get_feature_names()\n",
    "      \n",
    "    return X, vectorizer, ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bf002726fd445089e8e4d46cd800683",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=13500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "training vectorizer... ╯°□°）╯︵ ┻━┻\n",
      "vectorizer fit! (o_o)\n"
     ]
    }
   ],
   "source": [
    "X_skip_bigrams, skip_vectorizer, skipgram_names = skipgram_vectorizer(train, 2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_bigrams_vectorized = get_tfidf_ngrams(skip_vectorizer, test) # returns X_test for skipgrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_skip_bigrams\n",
    "X_test = skip_bigrams_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN_classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "KNN_classifier.fit(X_train, train_labels)\n",
    "KNN_prediction = KNN_classifier.predict(X_test)\n",
    "KNN_score = KNN_classifier.score(X_test, test_labels)\n",
    "KNN_f1 = f1_score(test_labels, KNN_prediction, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_classifier = DummyClassifier(strategy=\"prior\")\n",
    "dummy_classifier.fit(X_train, train_labels)\n",
    "dummy_prediction = dummy_classifier.predict(X_test)\n",
    "dummy_score = dummy_classifier.score(X_test, test_labels)\n",
    "dummy_f1 = f1_score(test_labels, dummy_prediction, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.72854410e-04, 1.53155723e-04, 6.49239403e-05, ...,\n",
       "       2.45455418e-05, 9.62685393e-04, 1.21588330e-04])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RandomForest_classifier = RandomForestClassifier(random_state=42)\n",
    "RandomForest_classifier.fit(X_train, train_labels)\n",
    "RandomForest_prediction = RandomForest_classifier.predict(X_test)\n",
    "RandomForest_score = RandomForest_classifier.score(X_test, test_labels)\n",
    "RandomForest_f1 = f1_score(test_labels, RandomForest_prediction, average=\"weighted\")\n",
    "RandomForest_classifier.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "NaiveBayes_classifier = GaussianNB()\n",
    "NaiveBayes_classifier.fit(X_train.toarray(), train_labels)\n",
    "NaiveBayes_prediction = NaiveBayes_classifier.predict(X_test.toarray())\n",
    "NaiveBayes_score = NaiveBayes_classifier.score(X_test.toarray(), test_labels)\n",
    "NaiveBayes_f1 = f1_score(test_labels, NaiveBayes_prediction, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_classifier = svm.SVC()\n",
    "SVM_classifier.fit(X_train, train_labels)\n",
    "SVM_prediction = SVM_classifier.predict(X_test)\n",
    "SVM_score = SVM_classifier.score(X_test, test_labels)\n",
    "SVM_f1 = f1_score(test_labels, SVM_prediction, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluation of Newspaper attribution\\n\")\n",
    "print(\"Accuracy scores:\\n KNN: {}\\n Dummy: {}\\n Random Forest: {}\\n Naive Bayes: {}\\n SVM: {} \\n\\n\".format(KNN_score, dummy_score, RandomForest_score, NaiveBayes_score, SVM_score))\n",
    "print(\"F1 scores:\\n KNN: {}\\n Dummy: {}\\n Random Forest: {}\\n Naive Bayes: {}\\n SVM: {} \\n\\n\".format(KNN_f1, dummy_f1, RandomForest_f1, NaiveBayes_f1, SVM_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
